To drag an entire text line up or down use: 
"start/alt/opt" + up 


To stop the running code: use shortcut Ctrl+Alt+M. or press F1 and then select/type Stop Code Run.


## From the Fast AI course
#Jupyter Notebooks:
press 'h' to pull up the shortcuts menu
#use formatted text which is done in 'markdown'
learn markdown its super easy and useful for jupyter Notebooks

#use this to insert website into markdown:
### The functions were copied from Matthew SF Choo's article [HERE](https://scientistwhostayed.medium.com/making-nlp-predictions-on-new-datasets-using-fast-ai-4a9be5e07ba1)




#look at last five entries in csv:

print(file.tail())



#Use zip() to subtract two lists
list1 = [2, 2, 2]
list2 = [1, 1, 1]
difference = [] initialization of result list.
zip_object = zip(list1, list2)
for list1_i, list2_i in zip_object:
difference. append(list1_i-list2_i) append each difference to list.
print(difference)


Making lists is faster this way:
everything = []
for chunk in list_of_lists:
    everything.extend(chunk)

this is the slower way:

for chunk  in list_of_lists:
    everything = everything + chunk

#write your own length function using a for loop to run throught the letters in a string:

name = 'donald'

def get_length(word):
  x=0
  for letter in word:
    x += 1
  return x

print(get_length(name))


#function to find if a letter is inside a string:
def letter_check(word, letter):
  for character in word:
    if character == letter:
      return True
  return False





# How to sort a list or tuple that is made up of lists and sort by a specific part of the smaller lists contained inside the larger list.
aka - "a complex object"
#A common pattern is to sort complex objects using some of the object’s indices as keys. For example:
>>>
>>> student_tuples = [
...     ('john', 'A', 15),
...     ('jane', 'B', 12),
...     ('dave', 'B', 10),
... ]
>>> sorted(student_tuples, key=lambda student: student[2])   # sort by age
[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]


#create a function that returns the first 3 of the first name with the first three of the last name
first_name = "Julie"
last_name = "Blevins"

def account_generator(first_name,last_name):
  account = first_name[:3] + last_name[:3]
  return account




#unpacking tuples:
#simple version
t= (0,1,3)  or t = 0,1,3
x, y, z = t

#(some dude)I prefer a list comprehension with a self-documenting tuple unpacking:

[x for x,y in training_data]
Complete program:
training_data=[(1.5, 11), (2.5, 22), (7.5, 77)]
training_data_x = [x for x,y in training_data]
print(training_data_x)

#cool list comprehension to have decimals:
possible_ms = [m * 0.1 for m in range(-100, 101)]

#cool way to use range to make x_values are the same as your list of months?:
months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]
range(len(months))
works great for plots: x_values = range(len(months))

#for making a list from a df column
col_one_list = df['column_name1'].tolist()


# Splitting strings:

'\n' Newline
'\t' Horizontal Tab
use 
chorus_lines = smooth_chorus.split('\n')

 #By adding a backslash in front of the special character we want to escape, \", we can include it in a string.

## Print notes
#how to print with a line inserted:
print('The word "computer" was used in Jeopardy questions: ' + '\n' + str(ninties_filtered_questions.answer.count()) + ' times in the 1990s' +'\n' + str(oughts_filtered_questions.answer.count()) + ' times in the 2000s' )
#this works to print two data frames at the same time. not mergedd:
print(df_1, df_2)

#useful:  .upper(), .lower(), .title(), .split(), .join(), and .format(), and .find()
.upper(), .title(), and .lower() adjust the casing of your string.
.split() takes a string and creates a list of substrings.
.join() takes a list of strings and creates a string.
.strip() cleans off whitespace, or other noise from the beginning and end of a string.
.replace() replaces all instances of a character/string in a string with another character/string.
.find() searches a string for a character/string and returns the index value that character/string is found at.
.format() and f-strings allow you to interpolate a string with variables.
#using .lower()
df['x'].str.lower()  
#or this works if its already a string
df.['x'] = df['x'].lower()

I used this with munchkin df:

#format the columns all columns into title case to match template
# tx_cl.str.title()

tx_cl.apply(lambda x: x.astype(str).str.title())

# turn the state column back into UPPER
tx_cl.state.astype(str).str.upper()



-
##How do we use the Python string formatters %d and %f?
Answer
In Python, string formatters are essentially placeholders that 
let us pass in different values into some formatted string.

The %d formatter is used to input decimal values, 
or whole numbers. If you provide a float value, it will convert it to a whole number, by truncating the values after the decimal point.

For example,

"print %d" % (3.78) 
# This would output 3

num1 = 5
num2 = 10
"%d + %d is equal to %d" % (num1, num2, num1 + num2)
# This would output 
# 5 + 10 is equal to 15
The %f formatter is used to input float values, or numbers with values after the decimal place. This formatter allows us to specify a number of decimal places to round the values by.

For example,

number = 3.1415
print "%f" % number
# You might see this output 3.1415000,
# due to how the float values are stored

print "%.3f" % number
# 3.142
# When specifying to a number of decimal places
# it will round up if the next digit value is >= 5

##Histogram:
The histogram above, for example(simple 10bar one), was created with the following code:

plt.hist(dataset) 
plt.show()
If we want more than 10 bins, we can use the keyword bins to set how many bins we want to divide the data into. The keyword range selects the minimum and maximum values to plot.
 For example, if we wanted to take our data from the last example and make a new histogram that just displayed the values from 66 to 69, divided into 40 bins (instead of 10),
  we could use this function call:

plt.hist(dataset, range=(66,69), bins=40)

-
#how to make 2 histograms so that you can see where they overlap:
alpha between 0 and 1 is the opacity

plt.hist(a, range=(55, 75), bins=20, alpha=0.5)
plt.hist(b, range=(55, 75), bins=20, alpha=0.5)
This would make both histograms visible on the plot:

use the keyword histtype with the argument 'step' to draw just the outline of a histogram:

plt.hist(a, range=(55, 75), bins=20, histtype='step')
plt.hist(b, range=(55, 75), bins=20, histtype='step')

Another problem we face is that our histograms might have different numbers of samples, making one much bigger than the other. We can see how this makes it difficult to compare qualitatively, by adding a dataset b with a much bigger size value:

a = normal(loc=64, scale=2, size=10000)
b = normal(loc=70, scale=2, size=100000)

plt.hist(a, range=(55, 75), bins=20)
plt.hist(b, range=(55, 75), bins=20)
plt.show()
The result is two histograms that are very difficult to compare:different_hist

To solve this, we can normalize our histograms using normed=True. This command divides the height of each column by a constant such that the total shaded area of the histogram sums to 1.

a = normal(loc=64, scale=2, size=10000)
b = normal(loc=70, scale=2, size=100000)

plt.hist(a, range=(55, 75), bins=20, alpha=0.5, normed=True)
plt.hist(b, range=(55, 75), bins=20, alpha=0.5, normed=True)
plt.show()

Now, we can more easily see the differences between the blue set and the orange set:normalized_hist



--





##Jeopardy filtering and lowercasing using all()
 # Filtering a dataset by a list of words

def filter_data(data, words):
  #lowercase all words in the list of words as well as the questions. ankik
  filter = lambda x: all(word.lower() in x.lower() for word in words)
  # Applies the labmda function to the Question column and returns the rows where the function returned True
  return data.loc[data["Question"].apply(filter)]

#this is what 'all()' is predefined to do:
def all(iterable):
    for element in iterable:
        if not element:
            return False
    return True



#how can you run a script in the terminal through vs code
you find the folder where it is. then type: python example.py   
it will run the file and print what you need in the terminal

#how can you make sure vs code is using the proper directory
Use the termnal to go the the directory that has the files you'd like to work in.  type: code . 
then press enter and it will open vscode in the directory you were looking for

#to print a df you MUST us the print function.  .head() function does not work on its own.  

#how to check what the data types are for each column
print(df.dtypes)

# Create a new list named double_nums by multiplying each number in nums by two. list comprehension
nums = [4, 8, 15, 16, 23, 42]
double_nums = [ i*2 for i in nums]

#user range in list comprehension:
nums = range(11)
squares = [ i ** 2 for i in nums] 

#good way to get lenght of strings
names = ["Elaine", "George", "Jerry", "Cosmo"]
lengths = [len(i) for i in names]

#using the NOT operator to flip booleans this is for a list called booleans
booleans = [True, False, True]
opposite = [not i for i in booleans]

#you can do this other boolean statements too:
greater_than_two = [ i > 2 for i in nums]

# how to create new_list will now contain the sum of each sub-list.
original_list = [[1, 2], [3, 4],  [5, 6]]
new_list = [item1 + item2 for (item1, item2) in original_list]

#Create a new list named first_character that contains the first character from every name in the list names
names = ["Elaine", "George", "Jerry", "Cosmo"]
first_character = [i[0] for i in names]

#using zip funciton in a list to divide separate lists together
a = [1.0, 2.0, 3.0]
b = [4.0, 5.0, 6.0]
quotients = [j/i for (i,j) in zip(a,b)]

#Create a new list named opposite that contains the opposite boolean for each element in the list booleans.
booleans = [True, False, True]
opposite = [not i for i in booleans]


# remove duplicates from a list:
my_list = [1,2,2,3,1,4,5,1,2,6]


my_finallist = []
[my_finallist.append(n) for n in my_list if n not in my_finallist] 
print(my_finallist)



### Data Frames ###


#Early pandas stuff
# Ways of creating a Pandas DataFrame

# Passing in a dictionary:

# set the column width max so that you can see the entire cell instead of trunkated.  othewise set it to 20 or something
pd.set_option('display.max_colwidth', -1)

data = {'name':['Anthony', 'Maria'], 'age':[30, 28]}
df = pd.DataFrame(data)

# Passing in a list of lists:
data = [['Tom', 20], ['Jack', 30], ['Meera', 25]]
df = pd.DataFrame(data, columns = ['Name', 'Age'])

# Reading data from a csv file:
df = pd.read_csv('students.csv')

#how do would you FILTER OUT all of the rows in df data frame "experimental group" so that you create a DF called a_clicks with only rows in experimental group A

b_clicks = ad_clicks[ad_clicks.experimental_group =='B']
        #or to filter by column comparison
crushing_it = sales_vs_targets[sales_vs_targets.revenue > sales_vs_targets.target]


#Filter out a list of names called some_values = [] from a df or dataframe from one column.
#this page explains all this really well: https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values

# df.loc[df['column_name'].isin(some_values)]

#filter all rows that contain this string:
ndfc.loc[ndfc['type'].str.contains(',')]

#filter all rows that contain this regex:
th = df[df['Region'].str.contains('th$')]

## Best site for filtering Pandas dfs:  https://datagy.io/filter-pandas/#select-dataframe-regex


#UPDATE: this is the dumb way for lots of names LOOK ABOVE for better way.filter out all states and counties I want to use for covid df use the example just below this line:
# df.query('col1 <= 1 & 1 <= col1'):
nyt_filtered = nyt.query('state == "texas" or state == "pennsylvania" or state == "colorado" or state == "california"')
nyt_filtered = nyt_filtered.query('county == "tarrant" or county == "philadelphia" or county == "san_diego" or county == "douglas"')

nyt_filtered = nyt_filtered.reset_index()



# to filter and sort use this:
nonunique_a = unique_a[unique_a.question > 24].sort_values(by='question',ascending=False)
roup
# to access or get the last element in a dataframe df
df["column"].iloc[-1]

# A function to find the unique answers of a set of data (instead of filtering)
def get_answer_counts(data):
    return data["Answer"].value_counts()

##how do you turn one element fromm a df into a variable.
    #Choose row and column of element
df2 = df.loc[(df.column1 == 'row_i_like'), ['column_i_like']]

##note: to do multiple columns:
df.loc[df['saleElapsed'] == df['saleElapsed'].max(), ['saleYear', 'saleMonth']]    
    
    
    
    
    #use .sum() to turn it into an integer
variable = df.column_i_like.sum()

## Sum multiple columns to one:
df['Fruit Total']= df.iloc[:,-4:-1].sum(axis=1)

    #Used to sum homohobia columns:
    k['sum_col'] = k[['toxic', 'severe_toxic', 'obscene', 'threat',
       'insult', 'identity_hate']].sum(axis=1)


# Specifying each value in the new column:
df['newColumn'] = [1, 2, 3, 4]

# Setting each row in the new column to the same value:
df['newColumn'] = 1

# Creating a new column by doing a 
# calculation on an existing column:
df['newColumn'] = df['oldColumn'] * 5

#new column from subraction of two other columns (simpler than it seems):
all_data['time_to_purchase'] = all_data.purchase_time - all_data.visit_time


#how do you rename dataframe columns?
df = df.rename(column={'old_nam':'new_nam'})

#Set the column labels to equal the values in the 2nd row (index location 1):
df.columns = df.iloc[1] 

## How to Handle datetime date Time Series Data with Ease:
This is really good: https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/09_timeseries.html

##from second covid iteration
# #find the date 8 days ago day
# from datetime import datetime, timedelta    
# d = datetime.today() - timedelta(days=1) #timedelta(days=days_to_subtract)

#make most recent column name and 1 week ago column name as variables to print out headers 
#from covid project
most_recent_date = deaths.columns[-1]
two_week_ago_date = deaths.columns[-8]

#from covid  
#use this to create the last index name or row name as a variable. useful after simple pivot
variable = df.index[-1]

#how to strip dataframe column headers or column names.  nifty way to clean up headers quickly:
#clean 

#How to append df.columns using lists and for loops:

list = nyt_filtered_san_diego.columns #make save all columns or column names into one list
listn =  [x + '_san_diego' for x in list] #add the same thing to each column name
nyt_filtered_san_diego.columns = listn #resave df.columns as your new list


## jeopardy.columns = jeopardy.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')

# jeopardy = jeopardy.rename(columns={' air_date':'air_date',' round':'round',' category':'category',' value': 'value',' question': 'question', ' answer': 'answer'})
##columns lambda x column lambda 
# jeopardy.rename(columns=lambda x: x.strip(), inplace=True)
jeopardy.columns = [col.lstrip() for col in jeopardy.columns]
# to turn all column headers to strings
# jeopardy.columns = [str(i) for i in jeopardy.columns.values.tolist()]

Trim leading space of column in pandas – lstrip()
Trim trailing space of column in pandas – rstrip()
Trim Both leading and trailing space of column in pandas – strip()
strip all the white space of column in pandas

##check your column headers or names:
print(df.columns)

## clean out any regular expressions like \n 

#replace both things with nothing
cols = [i.str.replace(r'\\n','').str.replace('xtms','') 



#ADD A NEW COLUMN with onto a df bassed off the df
df['newColumn'] = df['oldColumn'] * 5

#more complex version of the same gist. particularly weired because the pivot created columns that were headed by n
clicks_pivot['percent_clicked'] = \
   clicks_pivot[True] / \
   (clicks_pivot[True] + 
    clicks_pivot[False])


#cool lambda thing is that you can save a lambda like a function to a single variable name:

long_string = lambda str: len(str) > 12

print long_string("short")
print long_string("photosynthesis")

#for LAMBDAS use this for creating NEW COLUMN
# Applying to a row requires it to be called on the entire DataFrame.
# WAIT!! I think this is actually wrong.  
df['newColumn'] = df.apply(lambda row:      
  row['column1'] * 1.5 + row['column2'],
  axis=1)

#this lambda works though:


#interesting way to next if then statments into a lambda
nested_if_then_lambda = lambda x: '90s' if x<2 else (x**2 if x<4 else x+10)

#Create a column using a lambda on the one column with if then else 
#add prices to each coffee item_base_price
price_maker = lambda x: 18 if (x == 'ardi' or x =='yirgz') else 15
fake_box['item_base_price'] = fake_box.item_name.apply(price_maker)
print(fake_box)


#If the column ad_click_timestamp is not null, then someone actually clicked on the ad that was displayed.
#Create a new column called is_click, which is True if ad_click_timestamp is not null and False otherwise.

ad_clicks['is_click'] = ~ad_clicks.ad_click_timestamp.isnull()
#
#The ~ is a NOT operator, and isnull() tests whether or not the value of ad_click_timestamp is null.
#find how many are null in a particular column note you can use len to count number of rows
print(len(df[df.column1.isnull()]))


#list of pandas agregate functions:

df.columnName.sum()
df.columnName.mean() # Average of all values in column
df.columnName.std() # Standard deviation of column
df.columnName.median() # Median value of column
df.columnName.max() # Maximum value in column
df.columnName.min() # Minimum value in column
df.columnName.count() # Number of values in column
df.columnName.nunique() # Number of unique values in column
df.columnName.unique() # List of unique values in column
    note: use .drop_duplicates('col') if you want to filter only unique rows

#to change a whole column to a float use this:
df['DataFrame Column'] = df['DataFrame Column'].astype(float)

#way to clean a column of its Na values like Nan or Nat used in covid:
#turn Nan or in the float format into 0's 
#  df['DataFrame Column'] = df['DataFrame Column'].fillna(0)
death_merge.tarrant_nyt_deaths = death_merge.tarrant_nyt_deaths.fillna(0)

#cool way to clean all the numbers or strings first to make them all floats:
df = df_orig.copy()

#excellent catch all clean up solution from Practical Business Python.com
jeopardy['value'] = jeopardy['value'].apply(lambda x: x.replace('$', '').replace(',', '').replace('None','0')
                                if isinstance(x, str) else x).astype(float)
print(jeopardy.dtypes)
#note: you CAN use .apply to the entire df eg df.apply(lambda x: ...) that is actually the point here. 
to run this to clean up the entire df.


#how to detect if something is datetime try using this.  will be boolean so use in a lambda

ok so for some reason this worked:
isinstance(x, datetime)  NOT this: isinstance(now, datetime.datetime)   


#This was the equation that worked:
c_cov.iloc[:,1] = c_cov.iloc[:,1].apply(lambda x: x.parse()
                                if isinstance(x, datetime) else x)


##use the .apply() function to run a function through each element in a column:

def filter_desktop_mobile(platform):
    if platform in mobile:
        return 'Mobile'
    elif platform == 'Desktop':
        return 'Desktop'
    else:
        return 'Not Known'

data['platform'].apply(filter_desktop_mobile)



##using .apply() for multiple columns and specific rows:
Selecting multiple columns
To select multiple columns, you can pass a list of column names you want to select into the square brackets:

#here is how you can quickly create a list of your column names
note: you must trascribe col names to a list.  I don't think you can just use "df.columns[:]" by itself
col_names = df.columns[:]





# np.percentile can calculate any percentile over an array of values
#note: "np." is simply part of the 'command' so just copy it in
high_earners = df.
('category').wage
    .apply(lambda x: np.percentile(x, 75))
    .reset_index()

# grouping multiple colums aka performing aggregate :
df.groupby(['Location', 'Day of Week'])['Total Sales'].mean().reset_index()

#other example using id works well for "count."
shoe_counts = orders.groupby(['shoe_type', 'shoe_color']).id.count().reset_index()

#used for VA Governor Race to have only unique users and sum up the number per day for each candidate:
df.groupby(['day','polarity']).user.nunique().reset_index()




#super simple pivot  using .set_index This will move your entire table using a specific column as your index. 
#used in covid
df = df.set_index('column_name_to_use_as_index').T
dp = tarrant_df.set_index('county_name').T

#turns out that is called transposing and you can use these instead:
df_t = df.T
df_tr = df.transpose()
#more details here: https://note.nkmk.me/en/python-pandas-t-transpose/



#for pivoting aka rearranging tables(dataframes(df)):
df.pivot(columns='ColumnToPivot',
         index='ColumnToBeRows',
         values='ColumnToBeValues')


#specific example using pivot :
# First use the groupby statement:
unpivoted = df.groupby(['Location', 'Day of Week'])['Total Sales'].mean().reset_index()
# Now pivot the table
pivoted = unpivoted.pivot(
    columns='Day of Week',
    index='Location',
    values='Total Sales').reset_index()

#to do this all together with a print statement do this(formatted super clear):
print(df
    .groupby(['Location', 'Day of Week'])['Total Sales']\
    .mean()\
    .reset_index()\
    .pivot(\
        columns='Day of Week',
        index='Location',
        values='Total Sales')
    .reset_index()
    )

#other pivot example because your using the id.  ALSO 
shoe_counts = orders.groupby(['shoe_type', 'shoe_color']).id.count().reset_index()

shoe_counts_pivot = shoe_counts.pivot(columns='shoe_color',
         index='shoe_type',
         values='id').reset_index()

 # to print only 10 lines in pandas use
 print(user_visits.head(10))         

 
#How to sort a column in descending order or df based on one column

df2 = df.sort_values(by='column1',ascending = False)
# different sorting from the pandas site:
df.sort_values(by='col1', ascending=False)
df.sort_values(by='col1', ascending=False, na_position='first')
df.sort_values(by=['col1', 'col2'])
df.sort_values(by=['col1'])
 
nonunique_a = nonunique_a.sort_values(by=['question'],ascending=False)

#this is the sytax to print a sort a single column with a head to check something
print(jeopardy.sort_values(by=['air_date']).air_date.head(3))

This helped print from a .sort_values() when I kept getting This error: AttributeError: 'function' object has no attribute 'sort_values':
df.groupby('group')['id'].count().sort_values(ascending=False)


#using orderby or with ascending = false is also useful:
nonunique_a = unique_a[unique_a.question > 24].sort_values(by='question',ascending=False)




##modulo lambda to narrow any date into its decade:
#first clean data
    #make all dates liste  as strings into integers:
jeopardy['air_date'] = jeopardy['air_date'].apply(lambda x: x.replace('$', '')\
                                                        .replace(',', '')\
                                                        .replace('None','0')\
                                                        .replace('-','')
                                            if isinstance(x, str) else x).astype(int)
    
    #alternative cleaning for 'value' column
    #Fixes value column to be straight floats with no $'s commas or "None"
df['value_float'] = df.value.apply(lambda x: float(x[1:].replace(',','')) if x != 'None' else 0)


#then make all dates reduce to decade
jeopardy['decade'] = jeopardy.apply(lambda row:      
  ((row['air_date']-(row['air_date'] % 100000))/10000),
  axis=1)

    ##Add datetime column instead of string 'air_date'

    #must 'import datetime'
    import datetime as dt
    df['date'] = df.air_date.apply(lambda x: pd.to_datetime(x))
    #use it later like this
    filtered_by_computer_90s = filtered_by_computer[(filtered_by_computer.date > datetime.datetime(1990, 1, 1)) & (filtered_by_computer.date < datetime.datetime(1999, 12, 31))]
    
    #if looking to round dates to the 1st of the month do this:
    otb['month/year'] = pd.to_datetime(otb['month/year']) - pd.offsets.MonthBegin(0)
    #if looking to round up, do this
    otb['month/year'] = pd.to_datetime(otb['month/year']) + pd.offsets.MonthBegin(0)

    #interesting datetime or date or time way to import csv files:
        import codecademylib
        import pandas as pd

        visits = pd.read_csv('visits.csv',
                                parse_dates=[1])
        checkouts = pd.read_csv('checkouts.csv',
                                parse_dates=[1])
    #this allows you to use this time date subtraction later after merging the DataFrames:
    v_to_c['time'] = v_to_c.checkout_time - \
                 v_to_c.visit_time
    #We can use pandas parse_dates to parse columns as datetime. 
    You can either use parse_dates = True or parse_dates = [‘column name’]


ninties = jeopardy[jeopardy.decade == 1990.0]
oughts = jeopardy[jeopardy.decade == 2000.0]

#here I use the previouse data_filter() function
ninties_filtered_questions = data_filter(ninties, my_words)
oughts_filtered_questions = data_filter(oughts, my_words)

##usint input
>>> s = input('--> ')  
--> Monty Python's Flying Circus
>>> s  
"Monty Python's Flying Circus"


#how to select the last few columns or filter the specific number of columns in a df
 y = dataframe[dataframe.columns[-3:]]

## from covid project
#prints last 7 column entries
print(tarrant_df.iloc[0,-7:])
#prints the last column entry. note: .iloc[0,-1] only prints 2nd to last entry for like NO REASON
print(tarrant_df.iloc[0,-1:])

#anothr way to look at particualar data that does not use iloc is: this will give you what is 10 rows down
c_cov[1][10]
or 
c_cov['column_name'][10]


#using .loc or .iloc for index

# Single selections using iloc and DataFrame
# Rows:
data.iloc[0] # first row of data frame (Aleshia Tomkiewicz) - Note a Series data type output.
data.iloc[1] # second row of data frame (Evan Zigomalas)
data.iloc[-1] # last row of data frame (Mi Richan)
# Columns:
data.iloc[:,0] # first column of data frame (first_name)
data.iloc[:,1] # second column of data frame (last_name)
data.iloc[:,-1] # last column of data frame (id)

#or maybe this
The Pandas loc indexer can be used with DataFrames for two different use cases:

a.) Selecting rows by label/index
b.) Selecting rows with a boolean / conditional lookup
The loc indexer is used with the same syntax as iloc: data.loc[<row selection>, <column selection>] .

#other .loc or .iloc examples:

# select all rows with a condition
data.loc[data.age >= 15]
# select with multiple conditions
data.loc[(data.age >= 12) & (data.gender == 'M')]
#range of rows or a slice of the data
data.loc[1:3]

# ues .loc to select few columns with a condition
data.loc[(data.age >= 12), ['city', 'gender']]


# Select one row randomaly using sample() 
# without give any parameters 
df.sample() 
if using a column:
df.columnname.sample() 


## Merging two tables(inner merge?):
new_df = pd.merge(orders, customers)

big_df = orders.merge(customers)\
    .merge(products)

#easy way to merge DataFrames is to change the column names of one of the DataFrames:
    pets_owners = pd.merge(
        pets,
        ownner.rename(columns = {'id' : 'owner_id})
    )

#We could use the keywords left_on and right_on to specify which columns we want to perform the merge on. 
In the example below, the “left” table is the one that comes first (orders), and the “right” table is the one that comes
second (customers). This  syntax says that we should match the customer_id from orders to the id in customers.

pd.merge(
    orders,
    customers,
    left_on='customer_id',
    right_on='id')


#merge with suffix
pd.merge(
    orders,
    customers,
    left_on='customer_id',
    right_on='id',
    suffixes=['_order', '_customer']

you can perform a merge on one column, or on multiple specified columns, by passing in a list 
of the column names for each dataframe.

#When listing multiple column names, it will only return rows for which all the column values match. 
Furthermore, the number of columns listed must match, and the order they are listed will matter.
# This will match the values for 
# column "a" with "c" 
# and column "b" with "d".

pd.merge(
  df1,
  df2,
  left_on=["a", "b"],
  right_on=["c", "d"]
)

#outer merge
`pd.merge(company_a, company_b, how='outer')

#possible way to left merge:
df1.merge(df2, how='left')

#slick way of merging multiple df in one line
df1.merge(df2, how='left')\
   .merge(df3, how='left')

#Concatenate DataFrames
#When we need to reconstruct a single DataFrame from multiple smaller DataFrames, we can use the method 
# This method only works if all of the columns are the same in all of the DataFrames.
pd.concat([df1, df2, df2, ...])

#used this in munchkins.  NOTE! didn't work until I added axis=0, ignore_index=True 
#also note, i was concatenating a list of csv names
website_hours_scraped = pd.concat((pd.read_csv(f) for f in csvs), axis=0, ignore_index=True)

#how to drop duplicate rows in a dataframe df at least duplicate user id or the same entries
.drop_duplicates(subset='user_id')

#also this way to check duplicates aka the number of unique or .nunique()
print(len(cart))  # 400
print(  )  # 348
# There are 52 duplicates.

#other cool way to count duplicates when there are multiples of different entries:
my_dict = {i:MyList.count(i) for i in MyList}

>>> print my_dict     #or print(my_dict) in python-3.x
{'a': 3, 'c': 3, 'b': 1}


#
method I have used:
my_data.drop('Unnamed: 0', axis=1, inplace=True)
my_data.drop('Unnamed: 0', axis=1, drop=True) #this drops the old index and resets everything.

    df.drop('col1', axis=1, inplace=True)

#drop method alternate for dropping a column:
df.drop(subset = ['column_name'], axis=1)
#if you'd like it to perminently affect data use:
    df.drop(subset = ['column_name'], axis=1, inplace=true)
            ## might not need'subset = ' or brackets if only dropping entire columns or if axis=1?  I'm not sure yet
so whatever commmand you have to deal with the data it is o



#drop nan or missing values
df.dropna(how='any')

## Reset Index and drop old index
# reset the index and drop old index
#it is important too use inplace=True to make sure it doesn't just create a separate table
#It is imporant to drop=True to make sure python doesn't add the old index as a new column

    my_data.reset_index(inplace=True,drop=True)


## Pivot 
  b_pivot = b_clicks.groupby(['utm_source', 'is_click']).user_id.count().reset_index()\
  .pivot(columns='is_click',
        index='utm_source',
         values='user_id')\
  .reset_index()



## importing matplotlib
from matplotlib import pyplot as plt

#interesting bit on import and importing
import module_name or import module_name as alias import a whole module (in your case, pandas.py)
while "from" will import a function or a class of a module. So matplotlib is the module, and pyplot is very likely a class.
using different import styles has consequences.

#create simple line graph
x_values = [0, 1, 2, 3, 4]
y_values = [0, 1, 4, 9, 16]
plt.plot(x_values, y_values)
plt.show()


# Horizontal line at 0 create  a line at zero in graph
plt.hlines(y= 0,  xmin=xmin, xmax=xmax,color='red', linestyle ='dashed', linewidth = 4)



#if x and y do not have the same number of elements you can maybe do this:
If using the Numpy library, you can obtain the shape of an Array using the .shape property.

Items = np.array([1, 2, 3, 4, 5])
Items.shape # (5, ) - which means a 1D array of 5 elements

#change the line styles multiple lines
plt.plot(days, money_spent, color='green')
plt.plot(days, money_spent_2, color='#AAAAAA')

# Dashed:
plt.plot(x_values, y_values, linestyle='--')
# Dotted:
plt.plot(x_values, y_values, linestyle=':')
# No line:
plt.plot(x_values, y_values, linestyle='')

# A circle:
plt.plot(x_values, y_values, marker='o')
# A square:
plt.plot(x_values, y_values, marker='s')
# A star:
plt.plot(x_values, y_values, marker='*')

all the line styles here : https://matplotlib.org/api/lines_api.html

#for x= 0 to 3 and for y = 2 to 5:
x = [0, 1, 2, 3, 4]
y = [0, 1, 4, 9, 16]
plt.plot(x, y)
plt.axis([0, 3, 2, 5])
plt.show()

#naming labeling x an y axis
hours = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
happiness = [9.8, 9.9, 9.2, 8.6, 8.3, 9.0, 8.7, 9.1, 7.0, 6.4, 6.9, 7.5]
plt.plot(hours, happiness)
plt.xlabel('Time of day')
plt.ylabel('Happiness Rating (out of 10)')
plt.title('My Self-Reported Happiness While Awake')
plt.show()

#using subplots sub plots
# Data sets
x = [1, 2, 3, 4]
y = [1, 2, 3, 4]

# First Subplot
plt.subplot(1, 2, 1)
plt.plot(x, y, color='green')
plt.title('First Subplot')

# Second Subplot
plt.subplot(1, 2, 2)
plt.plot(x, y, color='steelblue')
plt.title('Second Subplot')

# Display both subplots
plt.show()


#sub plots I I 
plt.subplots_adjust() command. .subplots_adjust() has some keyword arguments that can move your plots within the figure:

left — the left-side margin, with a default of 0.125. You can increase this number to make room for a y-axis label
right — the right-side margin, with a default of 0.9. You can increase this to make more room for the figure, or decrease it to make room for a legend
bottom — the bottom margin, with a default of 0.1. You can increase this to make room for tick mark labels or an x-axis label
top — the top margin, with a default of 0.9
wspace — the horizontal space between adjacent subplots, with a default of 0.2
hspace — the vertical space between adjacent subplots, with a default of 0.2
For example, if we were adding space to the bottom of a graph by changing the bottom margin to 0.2 (instead of the default of 0.1), we would use the command:

plt.subplots_adjust(bottom=0.2)
We can also use multiple keyword arguments, if we need to adjust multiple margins. For instance, we could adjust both the top and the hspace:

plt.subplots_adjust(top=0.95, hspace=0.25)
Let’s use wspace to fix the figure above:

# Left Plot
plt.subplot(1, 2, 1)
plt.plot([-2, -1, 0, 1, 2], [4, 1, 0, 1, 4])

# Right Plot
plt.subplot(1, 2, 2)
plt.plot([-2, -1, 0, 1, 2], [4, 1, 0, 1, 4])

# Subplot Adjust
plt.subplots_adjust(wspace=0.35)

this also works
plt.subplots_adjust(wspace=0.35, bottom=0.2

plt.show()

These are the position values loc accepts and how to use it:
plt.legend(['parabola', 'cubic'], loc=6)
plt.show()

1 upper right
2 upper left
3 lower left
4 lower right
5  right
6 center left
7 center right
8 lower center
9 upper center
10 center



#label each line as you create it:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16],
         label="parabola")
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64],
         label="cubic")
plt.legend() # Still need this command!
plt.show()


#for modifying tic marks in specific sub plots in a figure
ax = plt.subplot(1, 1, 1)

Suppose we wanted to set our x-ticks to be at 1, 2, and 4. We would use the following code:

ax = plt.subplot()
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
ax.set_xticks([1, 2, 4])

with y tics

ax = plt.subplot()
plt.plot([1, 3, 3.5], [0.1, 0.6, 0.8], 'o')
ax.set_yticks([0.1, 0.6, 0.8])
ax.set_yticklabels(['10%', '60%', '80%'])

If your labels are particularly long, you can use the rotation keyword to rotate your labels by a specified number of degrees:
ax.set_xticklabels(['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto'],
rotation=30)

Full bar graph made here:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
ax = plt.subplot()
plt.bar(range(len(drinks)), sales)
ax.set_xticks(range(len(drinks)))
ax.set_xticklabels(drinks)
plt.show()

#Math for double bar graphs:

# China Data (blue bars)
n = 1  # This is our first d ataset (out of 2)
t = 2 # Number of datasets
d = 7 # Number of sets of bars
w = 0.8 # Width of each bar
x_values1 = [t*element + w*n for element
             in range(d)]

# US Data (orange bars)
n = 2  # This is our second dataset (out of 2)
t = 2 # Number of datasets
d = 7 # Number of sets of bars
w = 0.8 # Width of each bar
x_values2 = [t*element + w*n for element
             in range(d)]


#Double Bar example:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales1 =  [91, 76, 56, 66, 52, 27]
sales2 = [65, 82, 36, 68, 38, 40]

# sales1 data (blue bars)
n = 1  # This is our first dataset (out of 2)
t = 2 # Number of datasets
d = 6 # Number of sets of bars
w = 0.8 # Width of each bar
store1_x = [t*element + w*  for element
             in range(d)]
# plt.bar(range(len(days_in_year)),
#         days_in_year)
plt.bar(store1_x, sales1)

# sales2 dat (orange bars)
n = 2  # This is our second dataset (out of 2)
t = 2 # Number of datasets
d = 6 # Number of sets of bars
w = 0.8 # Width of each bar

store2_x = [t*element + w*n for element
             in range(d)]
plt.bar(store2_x, sales2)
plt.show()



#use to clear plots before new plot
plt.close('all') 

#to add a separate figure and change the size
plt.figure(figsize=(4, 10))


# Figure 2 save it
plt.figure(figsize=(4, 10)) 
plt.plot(x, parabola)
plt.savefig('tall_and_narrow.png')

For example, if the directory is a path within your current directory, you can save it like so,

plt.savefig('subfolder/filename.png')






#BAR CHART: Here is an example of how to make a bar chart using plt.bar to compare the number of days in a year on the different planets:
days_in_year = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
plt.bar(range(len(days_in_year)),
        days_in_year)
plt.show()



## how to download internet files through python used in covid project html csv:
    import urllib.request

    dls = "https://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyFatalityCountData.xlsx"
    urllib.request.urlretrieve(dls, "test.xlsx")

##convert xl or .xl or .xlsx to csv file
data_xls = pd.read_excel('excelfile.xlsx', 'Sheet2', index_col=None)
data_xls.to_csv('csvfile.csv', encoding='utf-8')



#converting to datetime date and time used in covid project
# overwriting data after changing format 
data["Date"]= pd.to_datetime(data["Date"])

##resets columns so some are now dates.
#df.columns = df.columns[:1].tolist() + pd.to_datetime(df.columns[1:]).tolist()
## use for days:
note: did not work on recent solar data
# df.columns = pd.to_datetime(df.columns).to_period('D')


## Create a 'Month' column from a date column
df['month'] = df['Date'].dt.strftime('%b')


##Iteresting date datetime speed notes:
def lookup(s):
    """
    This is an extremely fast approach to datetime parsing.
    For large data, the same dates are often repeated. Rather than
    re-parse these, we store all unique dates, parse them, and
    use a lookup to convert all dates.
    """
    dates = {date:pd.to_datetime(date) for date in s.unique()}
    return s.apply(lambda v: dates[v])

to_datetime: 5799 ms
dateutil:    5162 ms
strptime:    1651 ms
manual:       242 ms
lookup:        32 ms
Source: https://github.com/sanand0/benchmarks/tre



#combined better is:
c_cov.columns = c_cov.columns[:3].tolist() + pd.to_datetime(c_cov.columns[3:]).to_period('D').tolist()

#cool way to change the header or columns names and get rid of the old row with the old header. covid useful:
new_header = df.iloc[0] #grab the first row for the header
df = df[1:] #take the data less the header row
df.columns = new_header #set the header row as the df header


d1 = datetime.date(2008, 3, 12)

 #all datetime date time function are liste here:
https://pymotw.com/2/datetime/#:~:text=You%20can%20use%20datetime%20to,date%20to%20produce%20another%20date.


#how to convert columns from string to integer:
ou have three main options for converting types in pandas:

to_numeric() - provides functionality to safely convert non-numeric types (e.g. strings) to a suitable numeric type.
 (See also to_datetime() and to_timedelta().)
astype() - convert (almost) any type to (almost) any other type (even if it's not necessarily sensible to do so).
 Also allows you to convert to categorial types (very useful).
infer_objects() - a utility method to convert object columns holding Python objects to a pandas type if possible.

Read on for more detailed explanations and usage of each of these methods.

1. to_numeric()
The best way to convert one or more columns of a DataFrame to numeric values is to use pandas.to_numeric().

This function will try to change non-numeric objects (such as strings) into integers or floating point numbers as appropriate.

#really good stack-overflow explanation:
https://stackoverflow.com/questions/15891038/change-data-type-of-columns-in-pandas/44536326#:~:text=The%20best%20way%20to%20convert%20one%20or%20more%20columns%20of,floating%20point%20numbers%20as%20appropriate.


# convert Series
my_series = pd.to_numeric(my_series)

# convert column "a" of a DataFrame
df["a"] = pd.to_numeric(df["a"])

## SUPER IMORTANT  convert many columns (a list of specific columns) at once to_numeric or to_datetime if not all of them ##


# don't use .drop('column_name') if you want to convert entire df

cols = df.columns.drop('id')
df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')

does this work?
cols = df.columns.drop('id')
df[cols] = df[cols].apply(pd.to_numeric(downcast='integer'), errors='coerce')

exelent post here: https://stackoverflow.com/questions/36814100/pandas-to-numeric-for-multiple-columns

#to make things an integer
pd.to_numeric(s, downcast='integer')
#to make them float:
pd.to_numeric(s, downcast='flaot')

#apparently this can also change things from float to integer?
# df = df.astype({"a": int, "b": complex})

#this is what I used, but for some reason it would not work on the new 'test' column. it compiled,but did not 'test':

#make data integers:
tarrant_df.iloc[1:,2] = pd.to_numeric(tarrant_df.iloc[1:,2], downcast='integer')

## Used this to turn multiple columns from strings to numeric at once (note: for some reason an error keeps coming up about pasting something on a slice?)
# Tracker_sample[['product1','product2','product3','product4','Total']].apply(pd.to_numeric, errors='coerce').fillna(0)
nyt_pivot[['tarrant','philadelphia','san_diego','douglas']].apply(pd.to_numeric, errors='coerce')

#This is for loop worked better and I kept it in the covid thing to turn strings to numerics:
# note: the for loop allows for the '.fillna(0)' to work. while he example above cannot use .fillna(0)
# for col in ['product1','product2','product3','product4','Total']:
#     Tracker_sample[col] = pd.to_numeric(Tracker_sample[col],errors='coerce').fillna(0)


#create a test lambda:
tarrant_df['test'] = tarrant_df.iloc[1:,2].apply(lambda x: x + 1)


really good index for dataframe spot on how to search and whatnot or reset index:
https://www.kaggle.com/residentmario/indexing-selecting-assigning#Manipulating-the-index

#useful for printing out index number used in covid:
most_recent_date = tarrant_df.index[-1]

#worked for printing bar graph of all covid deaths. using date time as x axis. doesn't work like this unless using negatives(ie -150) because the first date is  off.
fig, ax = plt.subplots(figsize=(10, 6))
ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
ax.bar(tarrant_df.iloc[-150:,1], tarrant_df.iloc[-150:,2], width=0.3, align='center')


#great pandas examples for indexing with python like .loc and .iloc
https://pandas.pydata.org/pandas-docs/version/0.13.1/indexing.html


How to change an element in place in a column using if then:
https://datatofish.com/if-condition-in-pandas-dataframe/
import pandas as pd

numbers = {'set_of_numbers': [1,2,3,4,5,6,7,8,9,10,0,0]}
df = pd.DataFrame(numbers,columns=['set_of_numbers'])
print (df)

df.loc[df['set_of_numbers'] == 0, 'set_of_numbers'] = 999
df.loc[df['set_of_numbers'] == 5, 'set_of_numbers'] = 555

print (df)




##interesting simple way to pull data from a csv and have a dates column with adjustable x ticks
#came from this website: https://scentellegher.github.io/programming/2017/05/24/pandas-bar-plot-with-formatted-dates.html

#import libraries
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
%matplotlib inline

#read data from csv
data = pd.read_csv('data.csv', usecols=['date','count'], parse_dates=['date'])
#set date as index
data.set_index('date',inplace=True)

#set ggplot style
plt.style.use('ggplot')

#plot data
fig, ax = plt.subplots(figsize=(15,7))
ax.bar(data.index, data['count'])

#set ticks every week
ax.xaxis.set_major_locator(mdates.WeekdayLocator())
#format date
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))

ax.set_title('Game of Thrones Wikipedia Page Views')
ax.set_ylabel('Count')
ax.set_xlabel('Date')


#Interesting way to plot multiple subplots with a title in matplotlib 3.1.2
import matplotlib.pyplot as plt
import numpy as np


def f(t):
    s1 = np.cos(2*np.pi*t)
    e1 = np.exp(-t)
    return s1 * e1

t1 = np.arange(0.0, 5.0, 0.1)
t2 = np.arange(0.0, 5.0, 0.02)
t3 = np.arange(0.0, 2.0, 0.01)


fig, axs = plt.subplots(2, 1, constrained_layout=True)
axs[0].plot(t1, f(t1), 'o', t2, f(t2), '-')
axs[0].set_title('subplot 1')
axs[0].set_xlabel('distance (m)')
axs[0].set_ylabel('Damped oscillation')
fig.suptitle('This is a somewhat long figure title', fontsize=16)

axs[1].plot(t3, np.cos(2*np.pi*t3), '--')
axs[1].set_xlabel('time (s)')
axs[1].set_title('subplot 2')
axs[1].set_ylabel('Undamped')

plt.show()

##How I actually add a Title to the figure over all the plots for covid:
# Use pyplot.suptitle or Figure.suptitle:

# import matplotlib.pyplot as plt
# import numpy as np

# fig=plt.figure()
# data=np.arange(900).reshape((30,30))
# for i in range(1,5):
#     ax=fig.add_subplot(2,2,i)        
#     ax.imshow(data)

# fig.suptitle('Main title') # or plt.suptitle('Main title')
# plt.show()


##Error bars:
If we wanted to show an error of +/- 2, we would add the keyword yerr=2 to our plt.bar command. 
To make the caps wide and easy to read, we would add the keyword capsize=10:
values = [10, 13, 11, 15, 20]
yerr = 2
plt.bar(range(len(values)), values, yerr=yerr, capsize=10)
plt.show()

If we want a different amount of error for each bar, we can make yerr equal to a list rather than a single number:

values = [10, 13, 11, 15, 20]
yerr = [1, 3, 0.5, 2, 4]
plt.bar(range(len(values)), values, yerr=yerr, capsize=10)
plt.show() 

#How to do an error fill line aka a shade of error on a line graph matplotlib
Here is an example of how we would display data with an error of 2:

x_values = range(10)
y_values = [10, 12, 13, 13, 15, 19, 20, 22, 23, 29]
y_lower = [8, 10, 11, 11, 13, 17, 18, 20, 21, 27]
y_upper = [12, 14, 15, 15, 17, 21, 22, 24, 25, 31]

plt.fill_between(x_values, y_lower, y_upper, alpha=0.2) #this is the shaded error
plt.plot(x_values, y_values) #this is the line itself
plt.show()


In order to correctly add or subtract from a list, we need to use list comprehension:
y_lower = [i - 2 for i in y_values]


##Pie Chart! In Matplotlib, you can make a pie chart with the command plt.pie, passing in the values 
you want to chart:

budget_data = [500, 1000, 750, 300, 100]

plt.pie(budget_data)
plt.axis('equal')
plt.show()

When we make pie charts in Matplotlib, we almost always want to set the axes to be equal to fix this issue. 
To do this, we use plt.axis('equal'), 
which results in a chart like this: (a chart that looks better)


Example with Legend:
budget_data = [500, 1000, 750, 300, 100]
budget_categories = ['marketing', 'payroll', 'engineering', 'design', 'misc']

plt.pie(budget_data)
plt.legend(budget_categories)


One other useful labeling tool for pie charts is adding the percentage of the total that each slice occupies. 
Matplotlib can add this automatically with the keyword autopct. We pass in string formatting instructions to 
format the labels how we want. Some common formats are:

'%0.2f' — 2 decimal places, like 4.08
'%0.2f%%' — 2 decimal places, but with a percent sign at the end, like 4.08%. You need two consecutive percent signs because the first one acts as an escape character, so that the second one gets displayed on the chart.
'%d%%' — rounded to the nearest int and with a percent sign at the end, like 4%.
So, a full call to plt.pie might look like: ("pie chart with percentages inside pie and labels around it")

plt.pie(budget_data,
        labels=budget_categories,
        autopct='%0.1f%%')

--


##using the percent change built in pandas function:
percentages['percent_left'] = percentages.total.pct_change()

#my test df dataframe built quickly:
df_dic = {'samp': ['foo', "bar", 'bit', 'baz', 'bink'],
           'num' :  [23, 34, 456, 234, 8]
            }
df = pd.DataFrame(df_dic)
print(df)


#how to build your own data frame from dictionaries create new dataframe df :
df_dic = {'question': ['1. What are you looking for?', "2. What's your fit?", '3. Which shapes do you like?', '4. Which colors do you like?', '5. When was your last eye exam?'],
           'total' :  [500, 475, 380, 361, 270]
            }
                            
percentages = pd.DataFrame(df_dic)


#how to go back the last edit spot:
[command k then command q]



#example of iso datetime functions how to write datetime format:
datetime.datetime(2011, 11, 4, 0, 0)
>>> datetime.fromisoformat('2011-11-04T00:05:23')

#datetime.isoformat(sep='T', timespec='auto')
Return a string representing the date and time in ISO 8601 format:

YYYY-MM-DDTHH:MM:SS.ffffff, if microsecond is not 0

YYYY-MM-DDTHH:MM:SS, if microsecond is 0



#how to create a new df table with datetime
import pandas as pd
import numpy as np

np.random.seed(0)
# create an array of 5 dates starting at '2015-02-24', one per minute
rng = pd.date_range('2015-02-24', periods=5, freq='T')
df = pd.DataFrame({ 'Date': rng, 'Val': np.random.randn(len(rng)) }) 

print (df)
# Output:
#                  Date       Val
# 0 2015-02-24 00:00:00  1.764052
# 1 2015-02-24 00:01:00  0.400157
# 2 2015-02-24 00:02:00  0.978738
# 3 2015-02-24 00:03:00  2.240893
# 4 2015-02-24 00:04:00  1.867558

# create an array of 5 dates starting at '2015-02-24', one per day
rng = pd.date_range('2015-02-24', periods=5, freq='D')
df = pd.DataFrame({ 'Date': rng, 'Val' : np.random.randn(len(rng))}) 

print (df)
# Output:
#         Date       Val
# 0 2015-02-24 -0.977278
# 1 2015-02-25  0.950088
# 2 2015-02-26 -0.151357
# 3 2015-02-27 -0.103219
# 4 2015-02-28  0.410599

# create an array of 5 dates starting at '2015-02-24', one every 3 years
rng = pd.date_range('2015-02-24', periods=5, freq='3A')
df = pd.DataFrame({ 'Date': rng, 'Val' : np.random.randn(len(rng))})  

print (df)
# Output:
#         Date       Val
# 0 2015-12-31  0.144044
# 1 2018-12-31  1.454274
# 2 2021-12-31  0.761038
# 3 2024-12-31  0.121675
# 4 2027-12-31  0.443863

DataFrame with DatetimeIndex:

import pandas as pd
import numpy as np

np.random.seed(0)
rng = pd.date_range('2015-02-24', periods=5, freq='T')
df = pd.DataFrame({ 'Val' : np.random.randn(len(rng)) }, index=rng)  

print (df)
# Output:
#                           Val
# 2015-02-24 00:00:00  1.764052
# 2015-02-24 00:01:00  0.400157
# 2015-02-24 00:02:00  0.978738
# 2015-02-24 00:03:00  2.240893
# 2015-02-24 00:04:00  1.867558

Offset-aliases for parameter freq in date_range:

Alias     Description
B         business day frequency  
C         custom business day frequency (experimental)  
D         calendar day frequency  
W         weekly frequency  
M         month end frequency  
BM        business month end frequency  
CBM       custom business month end frequency  
MS        month start frequency  
BMS       business month start frequency  
CBMS      custom business month start frequency  
Q         quarter end frequency  
BQ        business quarter endfrequency  
QS        quarter start frequency  
BQS       business quarter start frequency  
A         year end frequency  
BA        business year end frequency  
AS        year start frequency  
BAS       business year start frequency  
BH        business hour frequency  
H         hourly frequency  
T, min    minutely frequency  
S         secondly frequency  
L, ms     milliseconds  
U, us     microseconds  
N         nanoseconds  



#easy way tocreate a practice df data frame:
df = pd.DataFrame({'A':[1,2,3],
               'B':[1,2,3],
               'C':[1,2,3],
               'D':[1,2,3]})    

#great page on using functiona and columns:
3


Is there a way in pandas to apply a function to a dataframe using the column names as argument names? 
https://stackoverflow.com/questions/58455054/python-pandas-apply-function-using-column-names-as-named-arguments#:~:text=4%20Answers&text=The%20function%20to%20apply%20f,a%20wrapper%20for%20this%20purpose.


##
##how to create face data to write code or queries to plug into real data

#generate genders, names, item_names
def random_genders(size, p=None):
    """Generate n-length ndarray of genders."""
    if not p:
        # default probabilities
        p = (0.49, 0.49, 0.01, 0.01)
    gender = ("M", "F", "O", "")
    return np.random.choice(gender, size=size, p=p)

#this is the example I took it from:
def random_genders(size, p=None):
    """Generate n-length ndarray of genders."""
    if not p:
        # default probabilities
        p = (0.49, 0.49, 0.01, 0.01)
    gender = ("M", "F", "O", "")
    return np.random.choice(gender, size=size, p=p)


#generate times





#more full explanation of the multiplication to make it day or minute or minute and seconds...
def random_datetimes_or_dates(start, end, out_format='datetime', n=10): 

    '''   
    unix timestamp is in ns by default. 
    I divide the unix time value by 10**9 to make it seconds (or 24*60*60*10**9 to make it days).
    The corresponding unit variable is passed to the pd.to_datetime function. 
    Values for the (divide_by, unit) pair to select is defined by the out_format parameter.
    for 1 -> out_format='datetime'
    for 2 -> out_format=anything else
    '''
    (divide_by, unit) = (10**9, 's') if out_format=='datetime' else (24*60*60*10**9, 'D')

    start_u = start.value//divide_by
    end_u = end.value//divide_by

    return pd.to_datetime(np.random.randint(start_u, end_u, n), unit=unit) 

#output of that:

start = pd.to_datetime('2015-01-01')
end = pd.to_datetime('2018-01-01')
random_dates(start, end)

DatetimeIndex(['2016-10-08 07:34:13', '2015-11-15 06:12:48',
               '2015-01-24 10:11:04', '2015-03-26 16:23:53',
               '2017-04-01 00:38:21', '2015-05-15 03:47:54',
               '2015-06-24 07:32:32', '2015-11-10 20:39:36',
               '2016-07-25 05:48:09', '2015-03-19 16:05:19'],
              dtype='datetime64[ns]', freq=None)




random_datetimes_or_dates(start, end, out_format='datetime')
DatetimeIndex(['2017-01-30 05:14:27', '2016-10-18 21:17:16',
               '2016-10-20 08:38:02', '2015-09-02 00:03:08',
               '2015-06-04 02:38:12', '2016-02-19 05:22:01',


                  '2015-11-06 10:37:10', '2017-12-17 03:26:02',
                   '2017-11-20 06:51:32', '2016-01-02 02:48:03'],
                  dtype='datetime64[ns]', freq=None)

random_datetimes_or_dates(start, end, out_format='not datetime')

DatetimeIndex(['2017-05-10', '2017-12-31', '2017-11-10', '2015-05-02',
               '2016-04-11', '2015-11-27', '2015-03-29', '2017-05-21',
               '2015-05-11', '2017-02-08'],
              dtype='datetime64[ns]', freq=None) 



## easy way to convert datetime to unix in nanoseconds:
pd.to_datetime('1970-01-01').value

remember to divide if you want days or seconds: days = x/24*60*60*10**9 , seconds = x/10**9

you can then use pd.to_datetime() to convert it back I think?


##how the .max() function works how to create a max() function how the .min() function works
# instead you can just use this:
#  most_recent_date = nyt_filtered_state['date'].max()

def biggest_date(thingy):
    max = x[0]
    for i in thingy:
        if i > max:
            max = i
    return max


##Both functions I used to create random datetimes and randomly chosen names, also, how to call them:

    def random_datetimes_or_dates(start='2020-01-01 00:00', end='2020-03-01 00:00', out_format='datetime', n=10):
        #set start and end to work from a string:
        fixed_start = pd.to_datetime(start)
        fixed_end = pd.to_datetime(end)
        '''set it up so it decides whether your using seconds or days.
        if seconds:
            date.value/10**9<--- this is how you get seconds from a unix timestamp aka date = pd.to_datetime('1936-01-01 00:05')
        if days:  
            date.value/24*60*60*10**9'''
        (divide_by, unit) = (10**9, 's') if out_format == 'datetime' else (24*60*60*10**9,'D')

        start_u = fixed_start.value// divide_by
        end_u = fixed_end.value// divide_by
        
        return pd.to_datetime(np.random.randint(start_u, end_u, n), unit=unit)


    #create randomish coffee box name function
    def item_name_randomizer(size, p=None):
        if not p:
            p = (.40, .40, .15, .05)
        names = ['ardi', 'yirgz', 'mexico', 'timor']
        return np.random.choice(names, p=p, size=size)
        
    #create df
    fake_box = pd.DataFrame()

    #create first column with coffee box func
    fake_box['item_name'] = item_name_randomizer(5, p=None)

    #create second column with dates
    # fake_box['created_at'] = random_datetimes_or_dates(start, end, out_format='datetime', n=5)
    fake_box['created_at'] =random_datetimes_or_dates(start='2020-01-01 00:00', end='2020-03-01 00:00', out_format='datetime', n=5)
    print(fake_box)


##how to tranfer table into an sqlite file:
#transfer to sqlite: from here: https://www.fullstackpython.com/blog/export-pandas-dataframes-sqlite-sqlalchemy.html

from sqlalchemy import create_engine

engine = create_engine('sqlite:///save_pandas.db', echo=True)
sqlite_connection = engine.connect()
sqlite_table = "fake_box_sales"
fake_box.to_sql(sqlite_table, sqlite_connection, if_exists='fail')

sqlite_connection.close()


##
using list comprehension in dataframes: https://chrisalbon.com/python/data_wrangling/pandas_list_comprehension/

#great place to see string manipulations: https://docs.python.org/3/library/string.html


As list comprehension
# Subtract 1 from row, for each row in df.year
`df['previous_year'] = [row-1 for row in df['year']]`

Otherwise you would do it as a for loop here:
# Create a variable
next_year = []

# For each row in df.years,
for row in df['year']:
    # Add 1 to the row and append it to next_year
    next_year.append(row + 1)

#nside password_generator create a for loop that iterates through the indices username by going from 0 to len(username).
#use a for loop to move the last letter of a string to the front (pretty good idea uing range that starts on -1)
def password_generator(user_name):
    password = ""
    for i in range(0, len(user_name)):
        password += user_name[i-1]
    return password


# Create df.next_year
df['next_year'] = next_year


#create a column of week total
nyt_filtered_tarrant['lsd_total_cases'] = nyt_filtered_tarrant.cases.diff(periods = 7)

#How to use the round() function in pandas for a column:

nyt_filtered_tarrant['cases_sd_avg'] = nyt_filtered_tarrant['cases_sd_total'] / 7.0
nyt_filtered_tarrant['cases_sd_avg'] = nyt_filtered_tarrant['cases_sd_avg'].round(1)


##data wrangling and tidying from codecademy ##


#.melt can be used to re-arrange datat so that some column end up  in rows while others remain as columns:
similar to pivot but a little more confusing
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html

df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
                   'B': {0: 1, 1: 3, 2: 5},
                   'C': {0: 2, 1: 4, 2: 6}})
df
   A  B  C
0  a  1  2
1  b  3  4
2  c  5  6

pd.melt(df, id_vars=['A'], value_vars=['B'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5


#easy ways to strip off website exess stuff like 'https\\'

# .str.lstrip('https://') removes the “https://” from the left side of the string
restaurants['url'] = restaurants['url'].str.lstrip('https://') 
 
# .str.lstrip('www.') removes the “www.” from the left side of the string
restaurants['url'] = restaurants['url'].str.lstrip('www.') 
 

#learned in family_covid_stats 
#how to create a new column using if then from two different columns 3 ways!!

    ##  these were the most basic examples:
    # #1
    # df['B']=df['A'].copy()
    # df.loc[df['A_type']!="String", 'B'] = "blank"
    # #2
    # df['B'] = df.apply(lambda x: x['A'] if x['A_type'] == 'String' else "Blank", axis = 1)
    # #3 & 4
    # df['B'] = df['A'].where(df['A_type'] == "String", 'blank')
    # #df['B'] = df['A'].mask(df['A_type'] != "String", 'blank')
    # #alternative
    # #df['B'] = np.where(df['A_type'] == "String", df['A'], 'blank')

    ##these were my examples
    #with
    # nyt_filtered_tarrant['new_where'] = nyt_filtered_tarrant.cases_sd_avg.where(nyt_filtered_tarrant.date == nyt_filtered_tarrant.date.max(), '0')
    #lamda
    # nyt_filtered_tarrant['new_lambda'] = nyt_filtered_tarrant.apply(lambda x: x['cases_sd_avg'] if x['date'] == nyt_filtered_tarrant.date.max() else '0', axis=1)
    #copy
    nyt_filtered_tarrant['new_copy'] = nyt_filtered_tarrant.cases_sd_avg.copy()
    nyt_filtered_tarrant.loc[nyt_filtered_tarrant.date != nyt_filtered_tarrant.date.max() , 'new_copy'] = '0'




##  From Data Viz Microcourse in Kaggle ##

    # setup jupyter notebook so you are using seaborn
    import pandas as pd
    pd.plotting.register_matplotlib_converters()
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    print("Setup Complete")

    # Path of the file to read
    spotify_filepath = "../input/spotify.csv"

    # Read the file into a variable spotify_data
    spotify_data = pd.read_csv(spotify_filepath, index_col="Date", parse_dates=True)

    # Line chart showing daily global streams of each song 
    sns.lineplot(data=spotify_data)

    # quick graph of a simple df
    import seaborn as sns
    sns.lineplot(y=dfrd.polarity, x=dfrd.rdate)




## how to set the size of the chart and change the title 

    # Set the width and height of the figure
    plt.figure(figsize=(14,6))

    # Add title
    plt.title("Daily Global Streams of Popular Songs in 2017-2018")

    # Line chart showing daily global streams of each song 
    sns.lineplot(data=spotify_data)



## How to print out two specific columns of a data set in seaborn 

    # Set the width and height of the figure
    plt.figure(figsize=(14,6))

    # Add title
    plt.title("Daily Global Streams of Popular Songs in 2017-2018")

    # Line chart showing daily global streams of 'Shape of You'
    sns.lineplot(data=spotify_data['Shape of You'], label="Shape of You")

    # Line chart showing daily global streams of 'Despacito'
    sns.lineplot(data=spotify_data['Despacito'], label="Despacito")

    # Add label for horizontal axis
    plt.xlabel("Date")



## Bar Chars & Heatmaps with Seaborn :  
    https://www.kaggle.com/alexisbcook/bar-charts-and-heatmaps

#Bar Chart:
    # Set the width and height of the figure
    plt.figure(figsize=(10,6))

    # Add title
    plt.title("Average Arrival Delay for Spirit Airlines Flights, by Month")

    # Bar chart showing average arrival delay for Spirit Airlines flights by month
    sns.barplot(x=flight_data.index, y=flight_data['NK'])

    # Add label for vertical axis
    plt.ylabel("Arrival delay (in minutes)")


## Heat Map:

#note: if you do not give it an x axis it will heat map all the info it has

    # Set the width and height of the figure
    plt.figure(figsize=(14,7))

    # Add title
    plt.title("Average Arrival Delay for Each Airline, by Month")

    # Heatmap showing 
     arrival delay for each airline by month
    sns.heatmap(data=flight_data, annot=True)

    # Add label for horizontal axis
    plt.xlabel("Airline")

## Scatter Plots:

# where: the horizontal x-axis (x=insurance_data['bmi']), and
#the vertical y-axis (y=insurance_data['charges']).

    sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'])

# with a regression line:
    sns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])

#w with third variable ie: bmi, charges, and smoker(boolean):

    sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'], hue=insurance_data['smoker'])

# now try with 2 linear regression lines. note the data= thing instesad of df['my_col'] situation:

    sns.lmplot(x="bmi", y="charges", hue="smoker", data=insurance_data)

# categorical scatter plot, and we build it with the sns.swarmplot command:

    sns.swarmplot(x=insurance_data['smoker'],
              y=insurance_data['charges'])


# Histogram 
    sns.distplot(a=iris_data['Petal Length (cm)'], kde=False)

note: We customize the behavior of the command with two additional pieces of information:

a= chooses the column we'd like to plot (in this case, we chose 'Petal Length (cm)').
kde=False is something we'll always provide when creating a histogram, as leaving it out will create a slightly different plot.

# KDE plots
    kernel density estimate (KDE) plot. In case you're not familiar with KDE plots, you can think of it as a smoothed histogram.

    To make a KDE plot, we use the sns.kdeplot command. 
    Setting shade=True colors the area below the curve (and data= has identical functionality as when we made the histogram above).

KDE plot#

    sns.kdeplot(data=iris_data['Petal Length (cm)'], shade=True)


# 2D KDE plot

    sns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind="kde")

# Histogram made up of multiple histograms from multiple files on one graph

    sns.distplot(a=iris_set_data['Petal Length (cm)'], label="Iris-setosa", kde=False)
    sns.distplot(a=iris_ver_data['Petal Length (cm)'], label="Iris-versicolor", kde=False)
    sns.distplot(a=iris_vir_data['Petal Length (cm)'], label="Iris-virginica", kde=False)

    # Add title
    plt.title("Histogram of Petal Lengths, by Species")

    # Force legend to appear
    plt.legend() #in case the legend doesn't automatically plot

## KDE plots for each species from multiple dfs

    sns.kdeplot(data=iris_set_data['Petal Length (cm)'], label="Iris-setosa", shade=True)
    
    
Good Summary of Different Graphs and Plots in Seaborn:
Since it's not always easy to decide how to best tell the story behind your data, we've broken the chart types into three broad categories to help with this.

Trends - A trend is defined as a pattern of change.

    sns.lineplot - Line charts are best to show trends over a period of time, and multiple lines can be used to show trends in more than one group.

Relationship - There are many different chart types that you can use to understand relationships between variables in your data.

    sns.barplot - Bar charts are useful for comparing quantities corresponding to different groups.
    sns.heatmap - Heatmaps can be used to find color-coded patterns in tables of numbers.

    sns.scatterplot - Scatter plots show the relationship between two continuous variables; if color-coded, we can also show the relationship with a third categorical variable.

    sns.regplot - Including a regression line in the scatter plot makes it easier to see any linear relationship between two variables.

    sns.lmplot - This command is useful for drawing multiple regression lines, if the scatter plot contains multiple, color-coded groups.

    sns.swarmplot - Categorical scatter plots show the relationship between a continuous variable and a categorical variable.

Distribution - We visualize distributions to show the possible values that we can expect to see in a variable, along with how likely they are.

    sns.distplot - Histograms show the distribution of a single numerical variable.

    sns.kdeplot - KDE plots (or 2D KDE plots) show an estimated, smooth distribution of a single numerical variable (or two numerical variables).

    sns.jointplot - This command is useful for simultaneously displaying a 2D KDE plot with the corresponding KDE plots for each individual variable.

Custom Styles:
    sns.set_style("darkgrid")

    different Seaborn styles:
    "darkgrid"
    "whitegrid"
    "dark"
    "white"
    "ticks"

    Use the style before the rest of the plotting:

    # Change the style of the figure
    sns.set_style("darkgrid")

    # Line chart 
    plt.figure(figsize=(12,6))
    sns.lineplot(data=spotify_data)

Kaggle Data sets:
    https://www.kaggle.com/datasets


## Cleaning Dad's Solar Exel sheet csv ##
``change columns header reset_index reset index
    ##  Here the columns were actually nul values and the first row had what I wanted as the columns.  Also there was a blank column[0] I wanted to get rid of.
    Also the dates might as well be the index so I might change that too.

my_data = pd.read_csv(my_filepath, parse_dates=True)

#Get rid of column 0
my_data = my_data.iloc[:,1:]

#save the row (row[0] that contains the preferred col names as a list:
new_header = my_data.iloc[0]

# Save the Date of the data:
date_sent = my_data.columns[0]

# get rid of 1st 3 rows
my_data = my_data.iloc[3:,:]

#clean off the bottom rows of the data:
my_data = my_data.iloc[:-13,:]

# reset the index and drop old index
my_data.reset_index(inplace=True,drop=True)

#change the column names to 'new_header':
my_data.columns = new_header
my_data.head()

#Make 'Date and Time' the index
    #notes:
        drop=True --> unnecessary.
        inplace=True --> ABSOLUTELY necesary, or it won't keep the fix
        substitution --> my_data = my_data.set_index('Date and Time')

my_data.set_index('Date and Time', inplace=True)




## reorder move change order columns rearrange##


best examples: https://stackoverflow.com/questions/35321812/move-column-in-pandas-dataframe

#easiest:

monthly_data = monthly_data[['Month'] + [c for c in monthly_data if c not in ['Month']]]


#good to make sure all the columns already exist

moving_cols = ['Date and Time','Month']
monthly_data = monthly_data[[c for c in moving_cols if c in monthly_data] + [c for c in monthly_data if c not in moving_cols]]

note: you cannot try to simply place a list like df = df[['my_list'] + [c for c in df]].  It will not work.  !! you must use a list comprehension eg [s for s in mylist]

## Create a month or a year column from a datetime column:
#first create month column in a monthly df

monthly_data = my_data


#I used the following in the munchkin project.  Just type names of the columns in the order you want and it works:

tx_cl = tx_cl[['school_name', 'phone_number', 'email', 'provider_type', 'address', 'city', 'state', 'zip','ages']]



#BEST WAY is .dt.month or .dt.year

monthly_data['Month'] = monthly_data['Date and Time'].dt.month


# alternative just for month, use dt.to_period()


# df['mnth_yr'] = df.date_column.dt.to_period('M') 
monthly_data['Month'] = monthly_data['Month'].dt.to_period('M') 



## a grouping key is a column or row that will be used to combine in a .groupby() operation. eg: df['Month] or df['blue']

##if you use df.groupby(['col']).mean() it will take any numerical columns and return their average based on 'col'.
if there isn't a numerical column it is called a 'nuisance' column    and will be excluded from the output.


##  Script for moving multiple xlsx sheets in one xl workbook  into multiple CSV files: ##
#from this site: https://medium.com/better-programming/using-python-to-convert-worksheets-in-an-excel-file-to-separate-csv-files-7dd406b652d7

'''This python script is to extract each sheet in an Excel workbook as a new csv file'''

import csv
import xlrd
import sys

def ExceltoCSV(excel_file, csv_file_base_path):
    workbook = xlrd.open_workbook(excel_file)
    for sheet_name in workbook.sheet_names():
        print('processing - ' + sheet_name)
        worksheet = workbook.sheet_by_name(sheet_name)
        csv_file_full_path = csv_file_base_path + str('_') + sheet_name.lower().replace(" - ", "_").replace(" ","_") + '.csv'
        csvfile = open(csv_file_full_path, 'w')
        writetocsv = csv.writer(csvfile, quoting = csv.QUOTE_ALL)
        for rownum in range(worksheet.nrows):
            writetocsv.writerow(
                list(x.encode('utf-8') if type(x) == type(u'') else x for x in worksheet.row_values(rownum)
                )
            )
        csvfile.close()
        print(sheet_name + ' has been saved at - ' + csv_file_full_path)
if __name__ == '__main__':
    ExceltoCSV(excel_file = sys.argv[1], csv_file_base_path = sys.argv[2])


##
To run this Python script, go to your terminal and do the following.
MacBook-Pro:~ bobthedude$ python exceltab_to_csv.py ./datafiles/client_data.xlsx ./datafiles/client_data_csv/






## Write CSV file and other things using PylightXl ##



# set the delimiter of the CSV to be the value of your choosing
# set the default worksheet to write the read in CSV data to
db = xl.readcsv(fn='input.csv', delimiter='/', ws='sh2')
# make modifications to it then,
# now write it back out as a csv; or as an excel file, see xl.writexl()
xl.writecsv(db=db, fn='new.csv', ws=('sh2'), delimiter=',')

## Read XLSX file
import pylightxl as xl

# readxl returns a pylightxl database that holds all worksheets and its data
db = xl.readxl(fn='folder1/folder2/excelfile.xlsx')

# read only selective sheetnames
db = xl.readxl(fn='folder1/folder2/excelfile.xlsx', ws=('Sheet1','Sheet3'))

# return all sheetnames
db.ws_names
>>> ['Sheet1', 'Sheet3']




##Reggies Linear Regression ##

#find the smallest error

possible_ms = [m * 0.1 for m in range(-100, 101)]
possible_bs = [b * 0.1 for b in range(-200, 201)]

datapoints = [(1, 2), (2, 0), (3, 4), (4, 4), (5, 3)]
smallest_error = float("inf")
best_m = 0
best_b = 0

for m in possible_ms:
    for b in possible_bs:
   	 error = calculate_all_error(m, b, datapoints)
   	 if error < smallest_error:
   		 best_m = m
   		 best_b = b
   		 smallest_error = error
       	 

print(best_m, best_b, smallest_error)



## Dad's Solar Project Learnings: ##

If there are 3 column headers: "col1\n", "col2xtms","col3\n"
how can you clean off the \n and why is it difficut:
Why difficult?
1.  the column names are in an index so you have to copy them as a list before using .replace()

2. you can't simply .replace the "\n" because it is a regular expression.  You must not only escape the regular expression ie('\\n' instead of '\n').
but you must also use r'\\n'.  r represents raw.  This tells python 2 things: a) read the following string as though it isn't a regular expression.  b) excape the '\' and read it as a raw string.  

I don't completely understand why you need both r and \ but you need both in Dad's solar example.

so to clean up:

#save the column names as a list called cols
cols = df.columns

#replace both things with nothing
cols = [i.str.replace(r'\\n','').str.replace('xtms','') 

#resave the list as the column headers
df.columns = cols


## This is how you create a column of datetime and coerce non-date times into NaT: ##
note: format indicates what format string is already in.


# df.date = pd.to_datetime(df.date, format='%Y%m%d', errors='coerce')

may.date = pd.(may.date,format='%Y%md',errors='coerce')

## This is how you take a column that is in datetime with seconds and
truncate it so it only shows the date.  

import datetime as dt


##  Drop any row with Na NaT or NaN using .dropna() ##
note: go here for excellent explanation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html


#use df.dropna(axis=0, how=any, inplace=True)
may.dropna(axis=0, how='any', inplace=True)


## How to extract the file name from a file path:
from:   https://www.kite.com/python/answers/how-to-extract-the-last-folder-or-the-file-from-a-path-in-python#:~:text=normpath(path)%20to%20remove%20extra,part%20of%20the%20original%20path.

a_path = '/Users/davidramsey/Documents/coding/solar/Oct_Meter_Readings_test/May'

without_extra_slash = os.path.normpath(a_path)  

last_part = os.path.basename(without_extra_slash)

print(last_part)


## how to save file names from a directory into a python list so you can iterate throught the list and clean each file- Google Search ##
#answer is here in both simple and better for memory versions: https://stackoverflow.com/questions/40371873/python-read-filenames-and-save-in-list

import os
import os.path

file_list = os.listdir("jsons") #import the filenames of everything in the directory

for file_name in file_list:
    #join the filename to the directory path then open it so you can mess with it. 
    with open(os.path.join("jsons", file_name), "r") as src_file: 
        #save it as a readable file then print the file
        data = src_file.read()
        print(data)


#Here is what I did on solar project:
dr_name = '/Users/davidramsey/Documents/coding/solar/Oct_Meter_Readings_test'
file_list = os.listdir('/Users/davidramsey/Documents/coding/solar/Oct_Meter_Readings_test')
for file in file_list:
    #connect the directory and file then open it so you can fuck with it
    with open(os.path.join(dr_name,file), mode = 'r') as src_file:
        #print something to confirm it's working
        print(str('this file!: ') + file)
        #close the file
        src_file.close()



#how does open() work in python - Google Search 

# opens the file in reading mode
f = open("path_to_file", mode='r')

# opens the file in writing mode 
f = open("path_to_file", mode = 'w')

# opens for writing to the end 
f = open("path_to_file", mode = 'a')

Python's default encoding is ASCII. You can easily change it by passing the encoding parameter.

f = open("path_to_file", mode = 'r', encoding='utf-8')



Debugging:  

Shortcuts:
F9: place a break in the line of code wher the curser is.

F5: to start debugging or iterate to the next stop
Shift+ F5 Stop debugging
F10: over to next line
F11: loop into deeper scope
Shift +F11: loop out of deeper scope into broader scope
Shift + CMD + F5 : rerun debugger fresh from the top
Shift + CMD + D = open debugging Run window

crtl +alt + left/right : split screen.  current tab moves to right.

FINALLY!! 
fn + start/alt/opt : gives you a multiple cursur!!!

If ever confuse:

1. slow down
2.  put a break point  -- F9
3. work through the lines of code one at a time
4. write out your variables
5.  print out your variables


---
explain the logic of Not True ect. with numbers. then remember the negation part
if True =1 False = 0
Not true = 0(false
1-1(true) = 0(false)
1-0(false = 1 (true)

you can also start getting wild.  If you substitute a probablility for True or false aka 1 or 0.  like .25

so NOT .25  goes to 1-.25 = .75
that it so far.




Define in which columns to look for missing values.
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html

df.dropna(subset=['name', 'toy'])
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT


# In solar data cleaner I used this to insert a list of columns to dropna():
#for a bit, then I changed it.
csv_file.dropna(subset=datelike_header, inplace=True)


# substituted .replace() all empty strings with na or Nan value so dropna() 
works cleaner.

    #sub all empty elements with NaN
    
    nan_value = float("NaN")
    
    # df.replace("",nan_value,inplace)
    csv_file[cols2].replace("", nan_value, inplace=True)



## Things That almost worked in solar cleaning with multiple columns
that had the word 'date' in them:

# # Clean out any na columns and reset index
    # # #use df.dropna(axis=0, how=any, inplace=True)
    # csv_file[datelike_header].dropna(axis=0, how='any', inplace=True)
    
    
    # df = df.loc[df['samp'].notnull()]
    
    # #create for loop so col_name is one variable not a list
    # for col_name in datelike_header:
    #     csv_file = csv_file[col_name].dropna()

    # csv_file.dropna(thresh=8,subset=datelike_header, inplace=True)

    
    
    # csv_file = csv_file[(csv_file['date'] != '')]
    
    
    # csv_file = csv_file[~csv_file[datelike_header].isin([''])]
    # df = df[(df[datelike_header] != '')]
    # csv_file = csv_file[(csv_file[datelike_header] != '')]

    
    # #reset the index of csv_file
    # csv_file = csv_file.reset_index(inplace=True, drop=True)
    
    ## drop the date column to make everything else numeric
    # cols = df.columns.drop('id') 
    # df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')



## Stuff I added to my notes to see if the columns matched between 
between solar csvs ##

#make old_cols equal cleaned columns:
    new_cols = cleaned_file.columns
    # compare old cols 
    
    #make sure you skip the the first csv as theres nothing to compare:
    if csv_count > 1:
        col_diff = new_cols.difference(old_cols)
    old_cols = cleaned_file.columns
    
    print(' Diff: ')
    print( col_diff)
    return old_cols



# good ways to join/merge two lists: https://thispointer.com/python-how-to-merge-two-or-more-lists/
#what I used:
all_cols.extend(new_cols)



# use pandas date_range() for creating a list of datetime entries:

pd.date_range(start='1/1/2018', end='1/08/2018')
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],
              dtype='datetime64[ns]', freq='D')


# Rearrange column order last column to first reorder order column:
cols = df.columns.tolist()
#note: these must be in reference to the rest of the list.
    #   like cols[1:]  NOT cols[3]
#move last column to first column
cols = cols[-1:] + cols[:-1]
df = df[cols]               


#how to check for date typos #

    #ignore this line if you alredy have typos:
    #create a typo:  change a date entry. subtact one year minus one year to the wrong year:
    df.loc[3, 'dates'] = df.loc[3, 'dates'] - pd.Timedelta(days=365)

    #create a column of the differences between dates:
    df['date_diff'] = df.dates.diff()
    #create a column that says true if dates are not within 1 day:
    df['double check'] = df['date_diff'] == pd.Timedelta(days=1)            


## how to print daterange in a column of datetime pandas  ##

    df[df.date.between('2016-01', '2016-06')]
    That gives:

        date
    0  2016-01
    1  2016-02

    It works because ISO 8601 date strings can be sorted as if they were plain strings. 
    '2016-06' comes after '2016-05' and so on.

    #Here is what I did to check solar date oddities:
    #it worked despite the dates already being in datetime type

    my_data = my_data[my_data['date_cleaned'].between('2020-01-14', '2020-01-18')]    



##use pd.to_datetime to save todays date as a variable:

note: this does not work on a series or col
x = pd.to_datetime('today').date()



## Ways to find the difference between dates or coloumn elements in succession: ##

.pct_change()
Dataframe.pct_change
Percent change over given number of periods.

.shift()
Dataframe.shift
Shift index by desired number of periods with an optional time freq.

.diff()
Series.diff
First discrete difference of object.


## how to use the .where() function:
m = df % 3 == 0
df.where(m, -df)
   A  B
0  0 -1
1 -2  3
2 -4 -5
3  6 -7
4 -8  9




##  Fast.AI ##
Markdown Here Cheatsheet:
https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet 



proxy = arrests
value = crime in the arrests for marijuana discussion


likely useful: 
1hr 7min in video:
cool thing about repl: type the name of a function into a spac e



Jupyter Notebook Key Shortcuts:
Shift+Enter:: Run the code or markdown on a cell

Up Arrow+Down Arrow:: Toggle across cells

b:: Create new cell

0+0:: Reset Kernel

You can find more shortcuts by typing h (for help).

Jupyter notebooks are great for experimenting and immediately seeing the results of each function, but there is also a lot of functionality to help you figure out how to use different functions, or even directly look at their source code. For instance, if you type in a cell:

??verify_images
a window will pop up with:

Signature: verify_images(fns)
Source:   
def verify_images(fns):
    "Find images in `fns` that can't be opened"
    return L(fns[i] for i,o in
             enumerate(parallel(verify_image, fns)) if not o)
File:      ~/git/fastai/fastai/vision/utils.py
Type:      function
This tells us what argument the function accepts (fns), then shows us the source code and the file it comes from. Looking at that source code, we can see it applies the function verify_image in parallel and only keeps the image files for which the result of that function is False, which is consistent with the doc string: it finds the images in fns that can't be opened.


## Jupyter Notebook Shortcuts:
Here are some other features that are very useful in Jupyter notebooks:

At any point, if you don't remember the exact spelling of a function or argument name, you can press Tab to get autocompletion suggestions.
When inside the parentheses of a function, pressing Shift and Tab simultaneously will display a window with the signature of the function and a short description. Pressing these keys twice will expand the documentation, and pressing them three times will open a full window with the same information at the bottom of your screen.

In a cell, typing ?func_name and executing will open a window with the signature of the function and a short description.

In a cell, typing ??func_name and executing will open a window with the signature of the function, a short description, and the source code.

If you are using the fastai library, we added a doc function for you: executing doc(func_name) in a cell will open a window with the signature of the function, a short description and links to the source code on GitHub and the full documentation of the function in the library docs.

Unrelated to the documentation but still very useful: to get help at any point if you get an error, type %debug in the next cell and execute to open the Python debugger, which will let you inspect the content of every variable.

A Bit of Deep Learning Jargon
Samuel was working in the 1960s, and since then terminology has changed. Here is the modern deep learning terminology for all the pieces we have discussed:

The functional form of the model is called its architecture (but be careful—sometimes people use model as a synonym of architecture, so this can get confusing).
The weights are called parameters.
The predictions are calculated from the independent variable, which is the data not including the labels.
The results of the model are called predictions.
The measure of performance is called the loss.
The loss depends not only on the predictions, but also the correct labels (also known as targets or the dependent variable); e.g., "dog" or "cat."/

-shortcuts from fast.ai:

to find out what a function is and where it comes from you can do a few things:
-doc()
like doc(path.ls)  
will show you the actual source code from fast.ai
-??path.ls()  will show you the purpose and parameters of the function
-path.ls?  will show a lot too at least maybe  

how to find out what type of tensor?
similar to python type()

you do tensor.type()
and it tells you what type of tensor it 







##Reddit PushShift PRAW PSAW API ##

#how to load a jason and read it if it's nested:

import json
# load data using Python JSON module
with open('data/nested_array.json','r') as f:
    data = json.loads(f.read())
# Flatten data
df_nested_list = pd.json_normalize(data, record_path =['students'])



how to load nd turn to dataframe if not nested from a URL:
URL = 'http://raw.githubusercontent.com/BindiChen/machine-learning/master/data-analysis/027-pandas-convert-json/data/simple.json'
df = pd.read_json(URL)

## create folder if it doesn't exist:
data_folder = "./data_files/"
file_name = "12-2020"

CHECK_FOLDER = os.path.isdir(data_folder)

# If folder doesn't exist, then create it.
if not CHECK_FOLDER:
    os.makedirs(data_folder)
    print("created folder : ", data_folder)




## regular expressions regex


w matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _, and the dash, -.
d matches digits, which means 0-9.
s matches whitespace characters, which include the tab, new line, carriage return, and space characters.
S matches non-whitespace characters.
. matches any character except the new line character n.



## Regex characters from https://www.fon.hum.uva.nl/praat/manual/Regular_expressions_1__Special_characters.html
The following characters are the meta characters that give special meaning to the regular expression search syntax:

\ the backslash escape character.
The backslash gives special meaning to the character following it. For example, the combination "\n" stands for the newline, one of the control characters. The combination "\w" stands for a "word" character, one of the convenience escape sequences while "\1" is one of the substitution special characters.
    Example: The regex "aa\n" tries to match two consecutive "a"s at the end of a line, inclusive the newline character itself.
    Example: "a\+" matches "a+" and not a series of one or "a"s.
^ the caret is the anchor for the start of the string, or the negation symbol.
    Example: "^a" matches "a" at the start of the string.
    Example: "[^0-9]" matches any non digit.
$ the dollar sign is the anchor for the end of the string.
    Example: "b$" matches a "b" at the end of a line.
    Example: "^$" matches the empty string.
{ } the opening and closing curly brackets are used as range quantifiers.
    Example: "a{2,3}" matches "aa" or "aaa".
[ ] the opening and closing square brackets define a character class to match a single character.
The "^" as the first character following the "[" negates, and the match is for the characters not listed. The "-" denotes a range of characters. Inside a "[ ]" character class construction, most special characters are interpreted as ordinary characters.
    Example: "[d-f]" is the same as "[def]" and matches "d", "e" or "f".
    Example: "[a-z]" matches any lower-case characters in the alphabet.
    Example: "[^0-9]" matches any character that is not an ASCII digit.
    Example: A search for "[][()?<>$^.*?^]" in the string "[]()?<>$^.*?^" followed by a replace string "r" has the result "rrrrrrrrrrrrr". Here the search string is one character class and all the meta characters are interpreted as ordinary characters without the need to escape them.
( ) the opening and closing parenthes3s are used for grouping characters (or other regexes).
The groups can be referenced in both the search and the substitution phase. There also exist some special constructs with parentheses.
    Example: "(ab)\1" matches "abab".
. the dot matches any character except the newline symbol.
    Example: ".a" matches two consecutive characters where the last one is "a".
    Example: ".*\.txt$" matches all strings that end in ".txt".
* the asterisk is the match-zero-or-more quantifier.
    Example: "^.*$" matches an entire line.
+ the plus sign is the match-one-or-more quantifier.
? the question mark is the match-zero-or-one quantifier. The question mark is also used in special constructs with parentheses and in changing match behaviour.
| the vertical pipe separates a series of alternatives.
    Example: "(a|b|c)a" matches "aa" or "ba" or "ca".
< > the smaller and greater signs are anchors that specify a left or right word boundary.
- the minus sign indicates a range in a character class (when it is not at the first position after the "[" opening bracket or the last position before the "]" closing bracket.
    Example: "[A-Z]" matches any uppercase character.
    Example: "[A-Z-]" or "[-A-Z]" match any uppercase character or "-".
& the ampersand is the "substitute complete match" symbol.

#Regex non-Pandas:

    text_after = re.sub(regex_search_term, regex_replacement, text_before)

    # my example:

    queryx = 'I suck at stuff:0'
    new = re.sub('\s','_', queryx)
    new

    ouput:  'I_suck_at_stuff:0'

 

## convert to 'raw' string:

fh = open(r"test_emails.txt", "r").read()

This technique converts a string into a raw string, which helps to avoid conflicts 

 caused by how some machines read characters, such as backslashes in directory paths on Windows.


how does this work?
for line in match:

    print(re.findall("\w\S*@", line))


#Concatenate a list of strings into one string: 
join()

## How combine multiple rows info into one row.

cols = ['foo', 'bar', 'new']
df['combined'] = df[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)




I used this with munchkin df:

#format the columns all columns into title case to match template
# tx_cl.str.title()

tx_cl.apply(lambda x: x.astype(str).str.title())

# turn the state column back into UPPER
tx_cl.state.astype(str).str.upper()

## how to find the length of a df column in pandas
len(df.index)
df.shape[0]
df[df.columns[0]].count() (slowest, but avoids counting NaN values in the first column)



#how to get rid of null values NaN so you can turn a column into integers
df = pd.DataFrame(Data)
df['Price'] = pd.to_numeric(df['Price'],errors='coerce')
df = df.replace(np.nan, 0, regex=True)
df['Price'] = df['Price'].astype(int)

print (df)
print (df.dtypes)

## Used on Solar to change dates:
note: no need to use r'asdlfd' simply write regex=True, BUT you MUST use regex=True
#locate rows where date < 2019-11-20 and use .replace() with regex to turn them to 2020 cause they wrooooong!
conds = df.date_cleaned<'2019-11-20'
m = df.loc[conds , 'date_cleaned'].replace('2019','2020', regex=True)




## bash stuff I just lear
ned before moving files 
ls -a	list all files including hidden file starting with '.'
ls --color	colored list [=always/never/auto]
ls -d	list directories - with ' */'
ls -F	add one char of */=>@| to enteries
ls -i	list file's inode index number
ls -l	list with long format - show permissions
ls -la	list long format including hidden files
ls -lh	list long format with readable file size
ls -ls	list with long format with file size
ls -r	list in reverse order
ls -R	list recursively directory tree
ls -s	list file size
ls -S	sort by file size
ls -t	sort by time & date
ls -X	sort by extension name

#This is how you get the first x number for files in your search with ls
#this is good for moving all new files in the downloads folder
ls -t1 ~/downloads | head -n 1

#this is apparently how you save them all to another direcrtory
    mv $(ls -t1 ~/downloads | head -n 1) ~/documents/my_new_directory

#here is how you move multiple dirctories into another directory:
    mv file1 file2 file3 -t DESTINATION
        or this

    mv -t <destination> <src1> <src2> .... <srnN>

mv -t DESTINATION file1 file2 file3 ( I think)

# Use this to sort ls by time and date recent files or csvs:
    ls -t *.csv

this is the page where it shows how to move a file from downloads
https://stackoverflow.com/questions/38288718/trying-to-write-a-shell-script-to-move-most-recent-file-from-downloads-to-anothe


##brew commands:
brew UPDATE
brew outdated
brew upgrade
https://youtu.be/SELYgZvAZbU?t=696


#remove all older versions of packages    

#diagnoses system and tells you about any issues it finds
brew doctor 




##Check which python version 
!python --version

#to see lots of info on a package
pip show selenium

use the ! to acces command line through jupyter notebook ipny or jnb

##amazing tutorials from this guy!:
http://jonathansoma.com/lede/foundations/classes/friday%20sessions/advanced-scraping-form-submissions-completed/



## Timeit Time Jupyter Notebook :
# to get a very accurate the time of an entire cell use:
# this will run it 7 times and giver you a standard deviation of how long it will take
# which is super good for extaplating how long it will take to run it 10 thousand times
by only running it 7 times.

%%timeit


# if you only want to run this 2x to keep it simple:
%%timeit #-r1

# if you just want to see how long the cell takes:
%%time

# note if you use %timeit with only one '%' you will time a specific line which can be separated by semicolons 
%time or %timeit


## if having trouble with permissions go here 
https://osxdaily.com/2020/04/27/fix-cron-permissions-macos-full-disk-access/


# cron and crontab scheduling running scripts:

Next, we need to add in a line describing the schedule frequency of how often we want the Python script to run. We input this in the order: minute, hour, day of the month, month, and day of the week. To leave one of these unspecified, place an asterisk (*) in that date / time slot.

Example 1: Run script on the first day of each month at 2:03.

1
3 2 1 * * python /path/to/test_script.py

from here http://theautomatic.net/2020/11/18/how-to-schedule-a-python-script-on-a-mac/


Also schedule specific fucntions in an application:

import schedule
 
def test():
    print("Test this out!")
 
schedule.every(10).seconds.do(test)
 
while True:
    schedule.run_pending()



## Scrapy:

#using to overwrite a file use capital 'O' to append to a file use lower case 'o'
(venv) (base) MacBook-Air-4:whiskyscraper3 davidramsey$ scrapy crawl whisky -O whisky.json

#really good POST stuff for NYC scraping with scrapy


##Regex used from nyc scraper:
these are the websites:

check your regex:
https://pythex.org/

tutorials:
https://regexone.com/





#Filtering Pandas nyc munchkin:
#Filter all rows/elements that contain this string:
#good site: https://datagy.io/filter-pandas/#select-dataframe-regex

ndfc.loc[ndfc['type'].str.contains(',')]


#Pandas regex used for recall gavin newsom:

rx = r'#\w*' # get all hashtags
df['regex'] = df.full_retweet_txt.str.findall(rx) #use findall() to save your regex as a separate column
df['regex'][:10] # check the first 10 entries of the new columm







## FastAI fast.ai:


# use to quickly see all colunns with specific names

' '.join(o for o in df.columns if o.startswith('sale'))


# use to create lost of info from just a date column:

df_test = add_datepart(df_test,'saledate')


# use Categorify and FillMissing to quickly turn categories into numbers 
and to fill missing values with the median value, then create a boolean column for "missing"

procs = [Categorify, FillMissing]


#  Split off a good test set from the test data using np.where and cond as a condition variable:
#   you want to only use year < Nov,2011




learned in chapter 9:

df.loc[df['saleElapsed'] == df['saleElapsed'].max(), ['saleYear', 'saleMonth']]
or 
cols = ['saleYear', 'saleMonth']
df.loc[df['saleElapsed'] == df['saleElapsed'].max(), cols]


# Interesting way to create a series of column names  you want to keep:
note: here fi.imp is fi['imp'] and its located inside fi.cols or fi['cols']

to_keep = fi[fi.imp>0.005].cols
len(to_keep)


## Beautiful dictionary comprehension to quickly see the results of many adjusted random forest regressions:
in this case it showed the OOB of the same df while dropping different columns individually:
-it simply prints out in the jupyter notebook.

{c: get_oob(xs_train_imp.drop(c, axis=1)) for c in ('saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',
    'fiModelDesc', 'fiBaseModel',
    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}



# Set all elements less than 1950 to 1950 in YearMade:

df.loc[df.YearMade<1950, 'YearMade'] = 1950


#  Grab a bunch of random indexes or

valid_idx = np.random.randint(842, size=(int(841/3)))
len(valid_idx)

note: what I actually used whas this:

from sklearn.model_selection import train_test_split
# separate valid and train rows
xs_train, xs_valid, y_train, y_valid = train_test_split(df, df.Survived, test_size=0.33, random_state=42)



## From 10_nlp FAst.AI ##
how to see as much of a text as you'd like.  
this shows the first 75 characters of  text element

txt = files[0].open().read(); txt[:75]

    'This movie, which I just discovered at the video store, has apparently sit '

# For Fast.AI homphobia project #
How to turn each element in a dataframe into individual .txt files for learning purposes:

## So.. super inefficient, but this worked perfectly.  weird, but worked:
# note: also consider itertuples() instead to preserve data types.

i = 0
for index, row in df.iterrows():
    if i>len(df.col):
        break
    else:
        f = open( str(i) + '.txt', 'w', encoding='utf-8')
        f.write(row[0])
        f.close()
        i+=1


# how to move files by opening bash in jupyter notebooks:
!pwd
!mv 9*.txt /notebooks/Homophobia


## Jupyter notebook file moving 


# create index lists for positive and negative stuff from the df readdit
cond = readdit.homophobic==1
neg_idx = np.where(cond)[0]
pos_idx = np.where(~cond)[0]
neg_idx[0]

# save the path to all the text files you want to sort
pos_path = Path('/notebooks/homophobia/train/pos')

# Make a list of all the paths to the text files
poses = get_text_files(pos_path)

#turn your numpy list from neg_idx to a string so it can be compared with your o.stem
negstr = [str(n) for n in neg_idx]

#re-path all text files in the 'pos' folder to be in the 'neg' folder if they are in the neg_idx list

for o in poses:
    if o.stem in negstr:
        npath = o.parent.parent / 'neg' / o.name
        o.replace(npath)
        
# Check to make sure the folders moved correctly 
!ls /notebooks/homophobia/train/neg


## used this to actually move text files around to train and valid folders:

negstrings = [str(o) for of in neg_idx] #turn numpy int64 o strings

test_path = Path('/notebooks/homophobia/test/')  #save the path
missing_text_files_paths = get_text_files(test_path)  #get all text files

for p in missing_text_files_paths:
    if p.stem in negstrings:
        #rename paths if the .stem is in the 'negstrings' list
        newest_path = p.parent.parent / 'train'/'neg'/p.name  
        p.replace(newest_path)
    else:
        #resave all other paths to the other folder
        newest_path = p.parent.parent / 'train'/'pos'/p.name  
        p.replace(newest_path)



## Path notes:
https://realpython.com/python-pathlib/#moving-and-deleting-files


from pathlib import Path


>>> path
PosixPath('/home/gahjelle/realpython/test.md')
>>> path.name
'test.md'
>>> path.stem
'test'
>>> path.suffix
'.md'

>>> path.parent
PosixPath('/home/gahjelle/realpython')
>>> path.parent.parent
PosixPath('/home/gahjelle')
>>> path.anchor
'/'



## show file tree

def tree(directory):
    print(f'+ {directory}')
    for path in sorted(directory.rglob('*')):
        depth = len(path.relative_to(directory).parts)
        spacer = '    ' * depth
        print(f'{spacer}+ {path.name}')

>>> tree(pathlib.Path.cwd())
+ /home/gahjelle/realpython
    + directory_1
        + file_a.md
    + directory_2
        + file_a.md
        + file_b.pdf
        + file_c.py
    + file_1.txt
    + file_2.txt



# Used to check number of files in directories in jupyter notbooks:

!ls /notebooks/homophobia_kaggle/train/neg | wc -l

# Remove directory dir and everything in it:
!rm ../homophobia_tok -r

#Linux stuff somewhere in the middle look up "mv"


# to turn any elements null that are over 3x the standard deviation of the rest of the column use this:
# did this for ariel  aeriel
ndf = adf[np.abs(adf - adf.mean()) <= (3 * adf.std())]

##Used to sum homohobia columns:
    k['sum_col'] = k[['toxic', 'severe_toxic', 'obscene', 'threat',
       'insult', 'identity_hate']].sum(axis=1)

# workds to concatenate dfs
## Used to add append duplicate replicate df rows:
    new_df = df[ df['sum_col'] > 0 ]
    df.append( [new_df] * 5, ignore_index=True)

#here repeaded_rows is a df appended to dfr_one:
    dfx = dfr_one.append(repeated_rows)


Used to filter df by list of words using .contains() and regex:

insults = ['gay', 'fag','queer', 'bugger', 'cock sucker','fαg','homo','hermaphrodite','lesbian', 'lez','closet']
hates = hates[hates['comment_text']\
              .str.lower().str.contains('|'.join(insults))]


# Label rows that contain text from a list:

df = df.loc[df.species.str.contains('cat|dog'), :]

    or




# how to speed up fastai?  maybe try these things on this page:
https://github.com/fastai/fastai/issues/2812

or look here:

https://forums.fast.ai/t/slow-dataloader-with-character-tokenizer/86552/4


## slick way of using numpy np.where() to do things with indexes and conditions or conds or cond

cond = (df.saleYear<2011) | (df.saleMonth<10)
train_idx = np.where( cond)[0]
valid_idx = np.where(~cond)[0]

splits = (list(train_idx),list(valid_idx))


    ## Real Example: use to drop unwanted rows:

    cond = (df.full_retweet_txt.str.contains('|'.join(o for o in posh))) & (df.full_retweet_txt.str.contains('|'.join(o for o in negh)))


    dropers = list(np.where(cond)[0])
    df = df.drop(dropers)




## Used this for filtering data frame with specific text:


## Shufle the row orders note: axis=1 would shuffle the columns I thingk

df.reindex(np.random.permutation(df.index))



This is useful for creating new columns:

    np.where(condition, value if condition is true, value if condition is false)

    df['hasimage'] = np.where(df['photos']!= '[]', True, False)



# Used to create new column:
# create your list of words to find
insults = ['gay', 'fag','queer', 'bugger','fαg','homo','hermaphrodite','lesbian', 'lez','closet']

# create your conditions for everything
cond = (k.identity_hate==1) & (k.comment_text.str.lower().str.contains('|'.join(o for o in insults)))

# create the column and place with either a 1 or a zero 
k['homp'] = np.where(cond, 1,0)


# example from abortion politics thing using & and | :
    cond = ((val.dpolarity<0) & (val.polarity<0)) | ((val.dpolarity>0) & (val.polarity>0))
    val['correct'] = np.where(cond, 1,0)

# Used to shuffle rows:
    sabh = abh.reindex(np.random.permutation(abh.index))


## command line bash: good for search and find:
!find / -name grizzly.jpg -print


# Text Clasifier Fastai example from someone:
https://github.com/dhamvi01/Fast.ai-Text-Classification/blob/main/Fast_ai_Text_Classification.ipynb



Twitter data:
Best of: https://imerit.net/blog/top-25-twitter-datasets-for-natural-language-processing-and-machine-learning-all-pbm/


top accounts
https://dataverse.harvard.edu/dataset.xhtml?id=3047332

sentiment from a kaggler:
https://www.kaggle.com/shashank1558/preprocessed-twitter-tweets?select=processedPositive.csv



## Tweepy Tutorial:
    # use this in jupyter notebooks to run a different notebook in the same directory:
    %run ./tweepy_passwds.py    

    # use this to find the five largest in a df column:
     mostlike = df.loc[df.likes.nlargest(5).index]
     print(mostlike)

    # had to load a specific packaged from spaCy to get it to work:
    !python3 -m spacy download en_core_web_lg

    # great easy way to count the numbers of things 
    df2 = df2[0].value_counts()

    #great shortcut for putting soemthing in quotes:
    hightlight word + ' or "

    # simple way to filter df:
    df8 = df6.where(df6['Entity'] == 'PERSON')


# Tweepy Search operators:
https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/guides/standard-operators

Standard search operators
The query can have operators that modify its behavior.  Below are examples that illustrate the available operators in standard search:

Operator	Finds Tweets...
watching now	containing both “watching” and “now”. This is the default operator.
“happy hour”	containing the exact phrase “happy hour”.
love OR hate	containing either “love” or “hate” (or both).
beer -root	containing “beer” but not “root”.
#haiku	containing the hashtag “haiku”.
from:interior	sent from Twitter account “interior”.
list:NASA/astronauts-in-space-now	sent from a Twitter account in the NASA list astronauts-in-space-now
to:NASA	a Tweet authored in reply to Twitter account “NASA”.
@NASA	mentioning Twitter account “NASA”.
politics filter:safe	containing “politics” with Tweets marked as potentially sensitive removed.
puppy filter:media	containing “puppy” and an image or video.
puppy -filter:retweets	containing “puppy”, filtering out retweets
puppy filter:native_video	containing “puppy” and an uploaded video, Amplify video, Periscope, or Vine.
puppy filter:periscope	containing “puppy” and a Periscope video URL.
puppy filter:vine	containing “puppy” and a Vine.
puppy filter:images	containing “puppy” and links identified as photos, including third parties such as Instagram.
puppy filter:twimg	containing “puppy” and a pic.twitter.com link representing one or more photos.
hilarious filter:links	containing “hilarious” and linking to URL.
puppy url:amazon	containing “puppy” and a URL with the word “amazon” anywhere within it.
superhero since:2015-12-21	containing “superhero” and sent since date “2015-12-21” (year-month-day).
puppy until:2015-12-21	containing “puppy” and sent before the date “2015-12-21”.
movie -scary :)	containing “movie”, but not “scary”, and with a positive attitude.
flight :(	containing “flight” and with a negative attitude.
traffic ?	containing “traffic” and asking a question.
Please, make sure to URL encode these queries before making the request. There are several online tools to help you to do that, or you can search at twitter.com/search and copy the encoded URL from the browser’s address bar. The table below shows some example mappings from search queries to URL encoded queries:



Search query	URL encoded query
#haiku #poetry	%23haiku+%23poetry
“happy hour” :)	%22happy%20hour%22%20%3A%29
Note that the space character can be represented by “%20” or “+” sign.

twitter searches used:
#vagov -from:GlennYoungkin -from:terrymcauliffe    all hashtag #vagov without the candidates.  note: no @ sign


# Presidential debate slick things:

lambda through a df 
def preprocess_tweets(tweet, custom_stopwords):
    preprocessed_tweet = tweet
    preprocessed_tweet.replace('[^/w/s]', '')
    preprocessed_tweet = " ".join(word for word in preprocessed_tweet.split() if word not in stop_words)
    preprocessed_tweet = " ".join(word for word in preprocessed_tweet.split() if word not in custom_stopwords)
    preprocessed_tweet = ' '.join(Word(word).lemmatize() for word in preprocessed_tweet.split())
    return(preprocessed_tweet)
    
df['processed_tweet'] = df['text'].apply(lambda x: preprocess_tweets(x, custom_stopwords))

Calculate sentiment:
df['polarity'] = df['processed_tweet'].apply(lambda x: TextBlob(x).sentiment[0])
df['subjectivity'] = df['processed_tweet'].apply(lambda x: TextBlob(x).sentiment[1])


# create rolling average with pandas:
        note: this creates a 10 tweet moving average with a minimum of 3 tweets to start
    dbiden['MA Polarity'] = biden['polarity'].rolling(10, min_periods=3).mean()



# cool way fo groupby or aggregating:
display(df[df['trump']==1][['trump', 'polarity','subjectivity']].groupby('trump')\
                        .agg([np.mean, np.max, np.min, np.median]),                     
df[df['biden']==1][['biden', 'polarity','subjectivity']].groupby('biden')\
                        .agg([np.mean, np.max, np.min, np.median])                                      
       )


# Good tweepy article for Streaming tweets:
https://towardsdatascience.com/real-time-twitter-sentiment-analysis-for-brand-improvement-and-topic-tracking-chapter-1-3-e02f7652d8ff

# Show entire string in jupyter notebook display:

    pd.options.display.max_rows
    pd.set_option('display.max_colwidth', -1)

#use .unique() to see all unique entries in pandas df 



# Creating a df from tweepy tweets  good way to create df from all columns available:
columns = set()
allowed_types = [str, int]
tweets_data = []
# test cursor for 1 tweet
for status in  cursor:
    status_dict = dict(vars(status))
    keys = vars(status).keys()
    single_tweet_data = {}
    for k in keys:
        try:
            v_type = type(status_dict[k])
        except:
            v_type = None
        if v_type != None:
            if v_type in allowed_types:
                single_tweet_data[k] = status_dict[k]
                columns.add(k)
    tweets_data.append(single_tweet_data)

header_cols = list(columns)

# Turning tensors into lists:

    the_tensor.tolist()
# to flatten a tensor and get all the values:
    the_tensor.flatten(.tolist())

#Count all values shown in an entire df 
df.apply(pd.Series.value_counts, normalize=True)

#Count all values in a column
df.colname.value_counts()


## expand the lists inside a column out into its own dataframe
# expand all lists of all hastags into df for counting
df2 = pd.DataFrame(dfn['regex'].values.tolist())



# import list of csv's to concat:

    import pathlib2 as pl2

    # concat all csvs
    path = pl.Path('/notebooks/clean/Tutorials') # save your dir path

    myf = [pl.Path(path/pl.Path(f)) for f in files] # create list of paths for all files

    dfs = [pd.read_csv(p) for p in myf] #load all csvs quickly

    df = pd.concat(dfs) #concat all quickly
    


df.set_index('timestamp', drop=True)


# Create convert columns with days or hours instead of exact time from timestamp:
note: automatically works with datetime

    df.timestamp = pd.to_datetime(df.timestamp)
    df['hour'] = df.timestamp.dt.to_period('H')
    df['day'] = df.timestamp.dt.to_period('D')


# create a simgle plot using pandas .plot().  
note: make sure you .set_index('timestamp') or something like that.

    fig, axs = plt.subplots(figsize=(12,4))
    dfx.plot()

# create multiple plots from the same df

    #turn to string cause its stupid and must be put back into normal datetime insted of 'hour' period.
    dfxc.hour = dfxc.hour.astype('str') 
    dfxc.hour = pd.to_datetime(dfxc['hour'])

    fig, axes = plt.subplots(2,1, figsize=(14,10))
    axes[0].plot(dfxc['hour'], dfxc['MA Polarity'])
    axes[0].set_title("\n".join(['Recall Polarity']))
    axes[1].plot(dfxc['hour'],dfxc['id'])
    axes[1].set_title("\n".join(['Tweets per Hour']))
    plt.show()


    # slick way of finding probability of a speciic thing:
        rate_est_samples = [...] # list of a specific number of(n_samples) elements thata are numbers
        sum(rate_est_samples > 0.4)/n_samples
        --> 0.1629 # all only samples larger than .4 divided by total is the probability of those samples.

#Pandas regex used for recall gavin newsom:

rx = r'#\w*' # get all hashtags
df['regex'] = df.full_retweet_txt.str.findall(rx) #use findall() to save your regex as a separate column
df['regex'][:10] # check the first 10 entries of the new columm


df2 = pd.DataFrame(df['regex'].values.tolist())
df2 = df2.stack(dropna=True)
df2.value_counts()


#DAily number of tweets:
    192k @ 8k per hour


# sum(list) is the sum of all the trues in a list.  AWESome!!

    a_boolean_list = [True, False, True, False, False, True]

    true_count = sum(a_boolean_list)
    ---> 3


Stats:  a point estimate:  if 2 people buy out of 4 people. a point esitmate is that there is .5 chance of someone buying


Plot coin tosses using sns seaborn

sns.barplot(x=[0,1,2,3,4],
            y=five_tosses_dist.pmf([0,1,2,3,4]),
            color='blue').set_title('Binomial Distribution, n=5, p=1/4')





## Auto update .format() or f' for a list:
# also unpack the list in a cool way
# list of all hashtags to search
    all_vagov_hashts = ['winwithglenn',
        'glenntrumpkin',
        'voteyoungkin',
        'vagov',
        'parentsforyoungkin',
        'teamyoungkin',
        'teamterry',
        'mcauliffedelivers']

    #create your string to insert into the query for format substitutions so it automatically updates if the list gets longer
    hashstring = ['#{'+str(x)+'} OR' for x in range(len(all_vagov_hashts) -1)]
    query_string = ''
    for x in hashstring:  query_string = query_string +' '+ str(x)
    query_string = query_string + ' #{'+str(len(all_vagov_hashts)-1)+'}'
    query_string

    # Search
    hashtags = all_vagov_hashts

    pre_query = f'{query_string} OR terrymcauliffe OR glennyoungkin OR mcauliffe OR youngkin -from:glennyoungkin -from:terrymcauliffe'
    query = pre_query.format(*hashtags)
    num_tweets = 2000
    query


#cool way to see the worst hashtags:
df2[(df2<.005) & (df2>.003)]

#min retweets
pre_query = f'{query_string} min_retweets:1000'

# row does not contain 'bill' but DOES contain 'trillion'

# used this try: and except: to no throw a giant error when there are no downloads:
    df = extract_df(limit_handled(tweepy.Cursor(api.search, q=query,tweet_mode='extended',lang='en').items(num_tweets)))
    df_full = df
    try:
        df = df[['timestamp','full_retweet_txt', 'retweet_count','id', 'user','location']]
    except:
        pass
    df.tail(1)

# datetime now for csv saving:  this saves x as a string of the day.
    
    import datetime
    x = str(datetime.datetime.now().date())
    x
    --->'2021-10-09'

    other version:
    x = str(datetime.datetime.now().time())
    x
    ---> '13:15:21.060403'


# crontab or you might type chrontab:

#open it in terminal -vim:
crontab -e

# check your crontabs:
crontab -l

# using vim:
i  to start typing
esc to stop typing
:wq  once escaped this will write and save what you typed in vim

# check you crontab code:
    www.crontab.guru



BASH COMMAND LINE SHELL terminal:

# delet all mail from crontab:
~ mail
~ delete *
~ q
q
type help for help


# activate your envelope:
    conda activate myenv

    or

    conda deactivate    

# install
conda install numpy

#list
conda list

#search
conda search pandas


##
Mail Command               Description
-------------------------  --------------------------------------------
t [message list]           type message(s).
n                          goto and type next message.
e [message list]           edit message(s).
f [message list]           give head lines of messages.
d [message list]           delete message(s).
s [message list] <file>    append message(s) to file.
u [message list]           undelete message(s).
R [message list]           reply to message sender(s).
r [message list]           reply to message sender(s) and all recipients.
p [message list]           print message list.
pre [message list]         make messages go back to /var/mail.
m <recipient list>         mail to specific recipient(s).
q                          quit, saving unresolved messages in mbox.
x                          quit, do not remove system mailbox.
h                          print out active message headers.
!                          shell escape.
| [msglist] command        pipe message(s) to shell command.
pi [msglist] command       pipe message(s) to shell command.
cd [directory]             chdir to directory or home if none given
fi <file>                  switch to file (%=system inbox, %user=user's
                           system inbox).  + searches in your folder
                           directory for the file.
set variable[=value]       set Mail variable.






## Used to publish seaborn sns bar graph for va tweets auto update: from update graph post:
https://www.freecodecamp.org/news/how-to-create-auto-updating-data-visualizations-in-python-with-matplotlib-and-aws/

    import seaborn as sns
    import matplotlib.pyplot as plt

    # Set the width and height of the figure
    plt.figure(figsize=(8,6))

    # Add title

    # Bar chart showing average arrival delay for Spirit Airlines flights by month

    ax = sns.barplot(x=df2, y=df2.index)

    #title
    ax.set_title(f'Most Popular Hashtags: Daily Va Governors Race:   {tdate}')

    # Add label for  axis
    ax.set(xlabel='Relative Percentage of Hashtags Used')
    

    ################################################
    #Save the figure to our local machine
    ################################################
    plt.tight_layout()
    plt.savefig('va_hashtags.png',bbox_inches="tight")


## Two ways to filtery by number of entries in a list that is in a row element (va Governors tweets)

    df[df.regex.str.len()>10]

or

    import numpy as np
    df.loc[np.array(list(map(len,df.regex.values)))>1]

# turn index of a series or df into a list:

    all_hashtags = df2.index



# Good site for statistics and probabilities:

https://math.libretexts.org/Bookshelves/Applied_Mathematics/Book%3A_College_Mathematics_for_Everyday_Life_(Inigo_et_al)/03%3A_Probability/3.02%3A_Combining_Probabilities_with_And_and_Or

# cool premier league statistics and probabilities:
https://towardsdatascience.com/the-premier-league-so-far-5bf1d95afbc8


Use .isin() to filter dataframe rows onn a list of values: https://datascienceparichay.com/article/filter-dataframe-on-list-of-values/

df_filtered = df[df['Col1'].isin(allowed_values)]



## From TheScientistWhoStayed ##

# HIS Model Loading

from fastai.text.all import *
from fastai.text.learner import language_model_learner
from fastai_ulmfit.pretrained import *
url = 'http://bit.ly/ulmfit-dewiki'
tok = tokenizer_from_pretrained(url)
learn = language_model_from_pretrained(dls_lm, 
                                       url=url, 
                                       drop_mult=1, 
                                       metrics=[Perplexity(), 
                                                accuracy],
                                       wd=0.1, 
                                       path=path2, 
                                       backwards=True,
                                      )
learn_clas = text_classifier_learner(dls_clas, 
                                AWD_LSTM, 
                                drop_mult=1, 
                                metrics=[rmse], 
                                path=path2,
                               )
# training goes here...
# training completed

## His Infernce Prediciton Preds:


# Use .export() instead of .save()
# save into /models/awd_lstm_fully_trained_export
learn_clas.export('awd_lstm_fully_trained_export')

##
from fastai.text.all import *
import pandas as pd
from pathlib import Path

df_test = pd.read_csv('path/to/new_dataset.csv') # the text to predict on is in the `excerpt` header.

def get_learner_for_inference(export_model_path: str):
    learn    = load_learner(export_model_path)
    # ensure learner is loaded on gpu cuda
    learn.dls.cuda()
    
    print(f'Learner stored on {learn.dls.device.type}')
    return learn


def get_preds_from_series(s: pd.Series, learner: Learner):
    # test_dl creates a new testing DataLoader 
    # using the pd.Series of text inputs
    dl_test   = learner.dls.test_dl(s)
    preds     = learner.get_preds(dl=dl_test)
    
    return preds[0].numpy().flatten()    

learn, dls_clas = get_learner_for_inference('/path/to/awd_lstm_fully_trained_export')

preds = get_preds_from_series(s        = df_test.excerpt,
                              learner  = learn,
                              )

preds

'''
RESULT:
Learner stored on cuda

array([-1.099862  , -0.07779016, -0.31440812, -1.9729153 , -1.589001  ,
       -1.4138943 , -0.6368377 ], dtype=float32)
'''




# used to bring timestamp to day or hour:
df.timestamp = pd.to_datetime(df.timestamp)
df['hour'] = df.timestamp.dt.to_period('H')
df['day'] = df.timestamp.dt.to_period('D')

# Used to look a top five individual tweets from  a large dataframe
for o in df.sort_values(['retweet_count'])[:5].full_retweet_txt:
    print ('-' + o)


# Use this to sort ls by time and date recent files or csvs:

ls -t *.csv

#use this to insert website into markdown:

    ### The functions were copied from Matthew SF Choo's article [HERE](https://scientistwhostayed.medium.com/making-nlp-predictions-on-new-datasets-using-fast-ai-4a9be5e07ba1)


## Real Clear Politics Aggregtor from terminal

    rcp http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html --output general.csv
    rcp https://www.realclearpolitics.com/epolls/2021/governor/va/virginia_governor_youngkin_vs_mcauliffe-7373.html --output general.csv

    rcp https://www.realclearpolitics.com/epolls/2021/governor/va/virginia_governor_youngkin_vs_mcauliffe-7373.html --output general.csv





# Ron Pacanowski NOtes

# BASH notes:
# jumps back to previous directory
cd - 

.bashrc

# Fast editor:

bearbones editor 


--

# used to get bautiful soup to work when it was returning 403 error.
# note: it needs a header to look like a browser.  so save header and headers = headers
# import requests

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'} # This is chrome, you can set whatever browser you like
response = requests.get('https://www.realclearpolitics.com/epolls/2021/governor/va/virginia_governor_youngkin_vs_mcauliffe-7373.html', headers=headers)

print(response.status_code)
print(response.url)

# 200 
# http://www.rlsnet.ru/search_result.htm?word=%D6%E5%F0%E5%E1%F0%EE%EB%E8%E7%E8%ED



#here repeaded_rows is a df appended to dfr_one:
    dfx = dfr_one.append(repeated_rows)


## cumcount to enumerate groups in a df

# create a column by enumerating over each poll repition. to get all the dates of the poll
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html
dfr['poll_day'] = dfr.groupby('index').cumcount()

    #this is essentially the same as :
        self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))
        or 
        df['newcol'] = df.groupby('col1').apply(lambda x: pd.Series(np.arange(len(x), x.index)))




# df_test = add_datepart(df_test, 'saledate')
dfr = add_datepart(dfr, 'end')
dfr = add_datepart(dfr, 'start')


# quick graph of a simple df
    import seaborn as sns
    sns.lineplot(y=dfrd.polarity, x=dfrd.rdate)


## Sleek Moving Average MA Polarity way to create graphing function from kaggle person:

    import datetime
    import matplotlib.pyplot as plt
    %matplotlib inline

    def show_ma_chart(df):
        ma1=75
        ma2=100
        
        ma_1 = pd.Series.rolling(df.Price, window=ma1).mean()
        ma_2 = pd.Series.rolling(df.Price, window=ma2).mean()
    
        xdate = [x for x in df.index]
    
        plt.figure(figsize=(15,5))
        plt.style.use('ggplot')
    
        plt.plot(xdate, df.Price, lw=1, color="black",label="Price")
        plt.plot(xdate, ma_1,lw=3,linestyle="dotted", label="Moving Average {} days".format(ma1))
        plt.plot(xdate, ma_2,lw=3,linestyle="dotted", label="Moving Average {} days".format(ma2))
        plt.legend(loc='best')
        plt.xlabel('Date')
        plt.ylabel('Price')
        xmin = df.index[0]
        xmax = df.index[-1]
        plt.title("Cushing_OK_WTI_Spot_Price_FOB ({0} to {1})".format(xmin.date(),xmax.date()))
        plt.ylim(0, 100)
        plt.xlim(xmin, xmax)
        plt.show()



    # Prepare dataset
    # The U.S. Energy Information Administration (EIA)
    df = pd.read_csv("../input/cushing-ok-wti-spot-price-fob/Cushing_OK_WTI_Spot_Price_FOB.csv", header=4, parse_dates=[0],index_col=[0],names=["Date","Price"])
    df.sort_index(inplace=True)

    # Show Moving Average
    show_ma_chart(df['2015-01-01':])
    show_ma_chart(df['2019-01-01':])



## Great NYT Article on polling and differences
https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html

## Fill in Na or null or NaN or NaT values in df for plot:
#pd.DataFrame
df['a_col_ffill'] = df['a_col'].ffill()
df['b_col_ffill'] = df['b_col'].ffill()  # changed from a to b
df[['a_col_ffill','b_col_ffill']].plot()


# create a single column df
df[['col']] will return a DataFrame. df['col'] will return a Series


# Horizontal line at 0
plt.hlines(y= 0,  xmin=xmin, xmax=xmax,color='red', linestyle ='dashed', linewidth = 4)
https://pythonguides.com/horizontal-line-matplotlib/



# convert timestamp to datetime
# note: watch out for dailight savings time.  
df['timestamp'] = pd.to_datetime(df.timestamp)
# create a CA time column
df['ca_time'] = df['timestamp'] - pd.Timedelta('7 hours')

# convert time from London time to VA time
df['timestamp'] = df['timestamp'] - pd.Timedelta('4 hours')


#RSME - root mean squared error

def rmse(predictions, targets):
    return np.sqrt(((predictions - targets) ** 2).mean())

# or

#create df without any days missing so you can don rmse
cdf = df[~df.dailyPolarityRCP_rpol.isnull()]
#use scikitlearn mse function
mean_squared_error(cdf.dailyPolarityRCP_rpol, cdf.dailyMaPolarityVa_bpol_ffill,squared=False)

# or R-squared

r2_score(cdf.dailyPolarityRCP_rpol, cdf.dailyMaPolarityVa_bpol_ffill)